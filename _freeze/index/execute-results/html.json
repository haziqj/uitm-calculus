{
  "hash": "808a4dfe5eedd6a5298237535d1eee5c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Calculus: Differentiation and Its Application'\nsubtitle: A statistical perspective\nformat:\n  ubd40-revealjs:\n    slide-level: 2\n    transition: fade\n    auto-stretch: false\n    width: 1250  # 1050\n    height: 760  # 700\n    self-contained: false\n    chalkboard: true\n    toc: false\n    toc-depth: 1\n    multiplex: true\n    code-block-height: 700px\n    # html-table-processing: none\nauthor:\n  - name: Dr. Haziq Jamil\n    orcid: 0000-0003-3298-1010\n    # email: haziq.jamil@ubd.edu.bn | `https://haziqj.ml/aiti-talk`\n    affiliations: \n      - 'Assistant Professor in Statistics, Universiti Brunei Darussalam'\n      - '<span style=\"font-style:normal;\">[`https://haziqj.ml/uitm-calculus/`](https://haziqj.ml/uitm-calculus/)</span>'\ndate: 2025-06-14\n# bibliography: refs.bib\nexecute:\n  echo: false\n  freeze: auto\n  cache: true\n---\n\n## (Almost) Everything you ought to know...\n\n### ...about calculus in the first year\n\n\n\nLet $f:\\mathcal X \\to \\mathbb R$ be a *real-valued* function defined on an input set $\\mathcal X$.\n\n::: {.fragment}\n\n::: {#def-derivatives}\n\n#### Differentiability\n\n$f(x)$ is said to be *differentiable* at a point $x \\in \\mathcal X$ if the limit \n\n$$\nL = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\n$${#eq-def-derivative}\nexists.\nIf $L$ exists, we denote it by $f'(x)$ or $\\frac{df}{dx}(x)$, and call it the *derivative* of $f$ at $x$.\nFurther, $f$ is said to be differentiable on $\\mathcal X$ if it is differentiable at every point in $\\mathcal X$.\n:::\n\n:::\n\n::: {.fragment}\nFor now, we assume $\\mathcal X \\subseteq \\mathbb R$, and will extend to higher dimensions later.\n:::\n\n## Some examples\n\n\n::: {.columns}\n\n::: {.column width=\"50%\"}\n<br>\n\n| Function | Derivative |\n| --- | --- |\n| $f(x) = x^2$ | $f'(x) = 2x$ |\n| $f(x) = \\sum_{n} a_n x^n$ | $f'(x) = \\sum_{n} n a_n x^{n-1}$ | \n| $f(x) = \\sin(x)$ | $f'(x) = \\cos(x)$ |\n| $f(x) = \\cos(x)$ | $f'(x) = -\\sin(x)$ |\n| $f(x) = e^x$ | $f'(x) = e^x$ |\n| $f(x) = \\ln(x)$ | $f'(x) = \\frac{1}{x}$ | \n| | |\n: {.table-ubd}\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"45%\"}\n\n<br>\n\nWe can derive it \"by hand\" using the definition.\nLet $f(x) = x^2$.\nThen,\n\n<br>\n<br>\n\n$$\n\\begin{align}\n\\lim_{h \\to 0} & \\frac{f(x + h) - f(x)}{h} \\\\\n&= \\lim_{h \\to 0} \\frac{(x + h)^2 - x^2}{h} \\\\\n&= \\lim_{h \\to 0} \\frac{x^2 + 2xh + h^2 - x^2}{h} \\\\[0.5em]\n&= \\lim_{h \\to 0} 2x + h \\\\[0.5em]\n&= 2x.\n\\end{align}\n$$\n\n:::\n\n:::\n\n::: aside\n[Taylor series](https://en.wikipedia.org/wiki/Taylor_series) is a powerful tool for approximating functions using derivatives. \n:::\n\n\n## Graphically...\n\n```{=html}\n<script src=\"https://www.desmos.com/api/v1.11/calculator.js?apiKey=dcb31709b452b1cf9dc26972add0fda6\"></script>\n\n<div id=\"desmos-derivative\" style=\"position: relative; width: 100%; height:88%;\"></div>\n\n<script>\n  var elt = document.getElementById(\"desmos-derivative\");\n  var calculator = Desmos.GraphingCalculator(elt, {\n    expressions: true,\n    settingsMenu: false,\n    zoomButtons: true\n  });\n\n  calculator.setExpressions([\n    { id: 'f', latex: 'f(x) = x^2' },\n    { id: 'a', latex: 'a = 1', sliderBounds: { min: -1, max: 3 } },\n    { id: 'h', latex: 'h = 1.5', sliderBounds: { min: 0.001, max: 1.5 } },\n    { id: 'pt1', latex: '(a, f(a))', showLabel: true },\n    { id: 'pt2', latex: '(a + h, f(a + h))', showLabel: true },\n    { id: 's', latex: 'L = \\\\frac{f(a+h) - f(a)}{h}' },\n    { id: 'tangent', latex: 'y = L(x - a) + f(a)', color: Desmos.Colors.RED }\n  ]);\n\n  calculator.setMathBounds({\n    left: -1,\n    right: 3,\n    bottom: -0.5,\n    top: 8\n  });\n\n</script>\n```\n\n## What is a derivative?\n\nThe derivative of a function tells you:    \n\n- üöÄ How fast the function is _changing_ at any point  \n- üìê The **slope** of the tangent line at that point\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-1-1.png){width=100%}\n:::\n:::\n\n\n## The concept of optimisation\n\n- When $f$ is some kind of a \"reward\" function, then the value of $x$ that maximises $f$ is highly of interest. Some examples:\n  - üí∞ **Profit maximisation**: Find the price that maximises profit.\n  - üß¨ **Biological processes**: Find the conditions that maximise growth or reproduction rates.\n  - üë∑‚Äç‚ôÇÔ∏è **Engineering**: Find the design parameters that maximise strength or efficiency.\n\n\n- Derivatives help us find so-called *critical values*: [Solve $f'(x) = 0$]{.ubdrubysoftbg}.\n\n::: {#exm-maxima}\n\nFind the maximum of $f(x) = -3x^4 + 4x^3 + 12x^2$.\n\n:::\n\n. . . \n\n$$\n\\begin{align*}\nf'(x) = -12x^3 + 12x^2 + 24x &= 0 \\\\\n\\Leftrightarrow 12x(2 + x - x^2) &= 0 \\\\\n\\Leftrightarrow 12x(x+1)(x-2) &= 0 \\\\\n\\Leftrightarrow x &= 0, -1, 2.\n\\end{align*}\n$$\n\n. . . \n\n::: {.nudge-up-medium}\nAre all of these critical values maxima values? ü§î\n:::\n\n## Graphically...\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-2-1.png){width=100%}\n:::\n:::\n\n\n\n## How do we know if it's a maxima or minima?\n\n[**Second derivative test:**]{.ubdrubydeep} Measure the **change in slope** around the critical point $\\hat x$, i.e. $f''(\\hat x) = \\frac{d}{dx}\\left( \\frac{df}{dx}(x) \\right) = \\frac{d^2f}{dx^2}(x)$.\n\n. . . \n\n::: {.nudge-down-medium}\n\n| Behaviour of $f$ near $\\hat x$               | $f''(\\hat x)$       | Shape             | Conclusion     |\n|-------------------------------|---------------------|----------------------|-------------------|----------------|\n| Increasing ‚Üí Decreasing       | $f''(\\hat x) < 0$ | Concave (‚à©)  | Local maximum  |\n| Decreasing ‚Üí Increasing       | $f''(\\hat x) > 0$ | Convex (‚à™)    | Local minimum  |\n| No sign change / flat region  |  $f''(\\hat x) = 0$   | Unknown / flat    | Inconclusive   |\n\n:::\n\n## Curvature\n\n::: {#def-curvature}\nLet $\\mathcal C_x$ denote the *osculating circle* at $x$ with centre $c$ and radius $r$, i.e. the circle that best approximates the graph of $f$ at $x$.\nThe curvature $\\kappa$ for a graph of a function $f$ at a point $x$ is defined as $\\kappa = \\frac{1}{r}$.\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/curvature-concav-1.png){width=100%}\n:::\n:::\n\n\n## Curvature and concavity\n\n::: {#def-curvature-alt}\n#### Curvature\nThe (signed) curvature for a graph $y=f(x)$ is\n$$\n\\kappa = \\frac{f''(x)}{\\big(1 + [f'(x)]^2\\big)^{3/2}}.\n$$\n:::\n\n- The second derivative $f''(x)$ tells us how fast the slope is changing.\n\n- The [**sign**]{.ubdrubydeep} of the curvature is the same as the sign of $f''(x)$. Hence,\n  - If $f''(x) > 0$, the graph is **concave up** (convex).\n  - If $f''(x) < 0$, the graph is **concave down** (concave).\n\n- The [**magnitude**]{.ubdrubydeep} of the curvature is proportional to $f''(x)$. Hence,\n  - If $|f''(x)|$ is large, the graph is *steep* and \"curvier\".\n  - If $|f''(x)|$ is small, the graph is *flat* and \"gentle\".\n  \n- For reference, a straight line has zero curvature.\n\n<!-- ```{r} -->\n<!-- #| fig-width: 7 -->\n<!-- #| fig-height: 3 -->\n<!-- #| out-width: 100% -->\n<!-- #| label: curvature-concav -->\n<!-- ``` -->\n\n## Summary so far\n\n- Derivatives represent rate of change (slope) of a function $f:\\mathcal X \\to \\mathbb R$.\n\n- Interested in optimising an *objective* function $f(x)$ representing some kind of \"reward\" or \"cost\".\n\n- Find critical points by solving $f'(x) = 0$.\n\n- Use the second derivative test to classify critical points:\n  - If $f''(x) < 0$, then $f$ is concave down at $x$ and $x$ is a local maximum.\n  - If $f''(x) > 0$, then $f$ is concave up at $x$ and $x$ is a local minimum.\n  - If $f''(x) = 0$, then the test is inconclusive.\n  \n- Curvature tells us how steep the curve is at its optima. In some sense, it tells us how hard or easy it is to find the optimum.\n\n# A statistical persepctive {.transition-slide-ubdrubydeep}\n\n## But what *is* statistics?\n\nStatistics is a scientific subject that deals with the collection, analysis, interpretation, and presentation of data.\n\n- **Collection** means designing experiments, questionnaires, sampling schemes, and also administration of data collection.\n\n- **Analysis** means mathematically modelling, estimation, testing, forecasting.\n\n::: {.nudge-up}\n![](figures/ppdac.png){width=100% fig-align=center}\n:::\n\n::: aside\nSee also: *The Art of Statistics: Learning from Data* by David Spiegelhalter.\n:::\n\n## Motivation\n\n> I toss a coin $n$ times and I wish to find $p$, the probability of heads. Let $X_i=1$ if a heads turns up, and $X_i=0$ if tails.\n\n- I do not know the value of $p$, so I want to estimate it somehow.\n\n- I have a \"guess\" what it might be e.g. $p=0.5$ or $p=0.7$.\n\n- How do I objectively decide which value is better?\n\n![](figures/bentcoin.png){.absolute right=10 top=200 width=\"220\"}\n\n\n\n\n::: {.callout-note}\n#### A more high-stakes example\n\nThink of the binary outcomes as a stock price rising or falling. \nYou'll need to decide to invest based on what you believe (or the data suggests) the probability of the stock price rising is.\n:::\n\n## A probabilistic model\n\nEach $X_i$ is a *random variable* taking only two possible outcomes, i.e.\n$$\nX_i = \\begin{cases}\n1 &\\text{w.p. } \\ \\ p \\\\\n0 &\\text{w.p. } \\ \\ 1-p \\\\\n\\end{cases}\n$$\nThis is known as a **Bernoulli** random variable.\n\n. . .\n\nSuppose that $X=X_1 + \\dots + X_n$ --- i.e. we are counting the number of heads in $n$ tossess --- then this becomes a **binomial** random variable. We write $X \\sim \\operatorname{Bin}(n, p)$, and the *probability mass function* is given by\n$$\nf(x \\mid p) = \\Pr(X = x) = \\binom{n}{x} p^x (1 - p)^{n - x}, \\quad x = 0, 1, \\ldots, n. \n$$\n\n## Learning from data\n\n::: {.callout-important}\n#### $p$ is unknown\nIf we do not know $p$, then it is not possible to calculate probabilities, expectations, variances... ‚òπÔ∏è\n:::\n\nNaturally, you go ahead and collect data by tossing it $n=10$ times. The outcome happens to be\n$$\nH, H, H, T, T, H, H, T, H, H\n$$\nThere is a total of $X=7$ heads, and from this you surmise that (at least) the coin is *unlikely* to be fair, because:\n\n- If $p=0.5$, then $\\Pr(X=7 \\mid p = 0.7) = \\binom{10}{7} (0.5)^7 (0.5)^3 = 0.117$.\n- If $p=0.7$, then $\\Pr(X=7 \\mid p = 0.5) = \\binom{10}{7} (0.7)^7 (0.3)^3 = 0.267$.\n- If $p=0.9$, then $\\Pr(X=7 \\mid p = 0.9) = \\binom{10}{7} (0.9)^7 (0.1)^3 = 0.057$.\n\n::: {.nudge-up-large}\nHow to formalise this idea?\n:::\n\n## The likelihood function\n\n::: {#def-likelihood}\nGiven a probability function $x \\mapsto f(x\\mid\\theta)$ where $x$ is a realisation of a random variable $X$, the *likelihood function* is $\\theta \\mapsto f(x\\mid\\theta)$, often written\n$\\mathcal L(\\theta) = f(x \\mid \\theta)$.\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-3-1.png){width=100%}\n:::\n:::\n\n\n\n## Parameteric statistical models\n\nAssume that $X_i \\sim f(x \\mid \\theta)$ independently for $i = 1, \\ldots, n$.\nHere, functional form of $f$ is known, but the parameter $\\theta$ is unknown. Examples:\n\n| Name        | $f(x \\mid \\theta)$                                                   | $\\theta$                                | Remarks                          |\n|-------------|----------------------------------------------------------------------|------------------------------------------|----------------------------------|\n| Binomial    | $\\binom{n}{x} p^x (1 - p)^{n - x}$                                   | $p \\in (0, 1)$        | No. successes in $n$ trials        |\n| Poisson     | $\\frac{\\lambda^x e^{-\\lambda}}{x!}$                                  | $\\lambda > 0$                            | Count data  |\n| Uniform     | $\\frac{1}{b - a}$ for $x \\in [a, b]$                                 | $a < b$                                  | Equally likely outcomes     |\n| Exponential | $\\lambda e^{-\\lambda x}$ for $x \\geq 0$                              | $\\lambda > 0$                            | Waiting time        |\n| Normal      | $\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$ | $\\mu \\in \\mathbb{R},\\ \\sigma^2 > 0$ | Bell curve       |\n|   |   |   |\n: {.table-ubd}\n\n\n## Example (Normal mean)\n\nSuppose we observe $X_1, \\ldots, X_n$ from a normal distribution with unknown mean $\\mu$ and known variance $\\sigma^2$.\nThe log-likelihood function is given by\n$$\n\\begin{align*}\n\\ell(\\mu) &= \\sum_{i=1}^n \\log f(X_i \\mid \\mu) \\\\\n&= \\sum_{i=1}^n \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(X_i - \\mu)^2}{2\\sigma^2}} \\right) \\\\\n&= \\sum_{i=1}^n \\left\\{ -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(X_i - \\mu)^2}{2\\sigma^2} \\right\\} \\\\\n&= -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (X_i - \\mu)^2.\n\\end{align*}\n$$\n\n## Example (Normal mean, cont.)\n\nTo find the MLE of $\\mu$, we differentiate the log-likelihood function with respect to $\\mu$ and set it to zero:\n$$\n\\begin{align*}\n\\frac{d}{d\\mu} \\ell(\\mu) &= -\\frac{1}{2\\sigma^2} \\cdot 2 \\sum_{i=1}^n (X_i - \\mu)(-1) \\\\\n&= \\frac{1}{\\sigma^2} \\sum_{i=1}^n (X_i - \\mu) = 0 \\\\\n&\\Leftrightarrow \\sum_{i=1}^n X_i - n\\mu = 0 \n\\Leftrightarrow \\mu = \\frac{1}{n} \\sum_{i=1}^n X_i.\n\\end{align*}\n$$\n\nThus, the MLE for $\\mu$ is $\\hat\\mu = \\bar X = \\frac{1}{n} \\sum_{i=1}^n X_i$.\n\n## A real data example\n\n::: {#exm-house-price}\nSample $n=50$ house prices randomly in Brunei.\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-4-1.png){width=100%}\n:::\n:::\n\n\n\n::: aside\nHJ (2025). A spatio-temporal analysis of house prices in Brunei Darussalam. *Qual Quant*, 1-32. DOI: [10.1007/s11135-025-02164-0](https://doi.org/10.1007/s11135-025-02164-0).\n:::\n\n\n## But wait, the sample was random...\n\n::: {.columns}\n\n::: {.column width=30%}\n::: {.nudge-down}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n254 242 367 310 347 197 173 220 166 309 287 ...\n```\n\n\n:::\n:::\n\n:::\n:::\n\n::: {.column width=20%}\n$\\bar X = 271.8$\n:::\n\n::: {.column width=50%}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-6-1.png){width=100%}\n:::\n:::\n\n:::\n\n:::\n\n\n\n::: {.columns}\n\n::: {.column width=30%}\n::: {.nudge-down}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n398 80 370 273 289 414 214 303 100 201 351 ...\n```\n\n\n:::\n:::\n\n:::\n:::\n\n::: {.column width=20%}\n$\\bar X = 270.7$\n:::\n\n::: {.column width=50%}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-8-1.png){width=100%}\n:::\n:::\n\n:::\n\n:::\n\n\n::: {.columns}\n\n::: {.column width=30%}\n::: {.nudge-down}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n239 285 191 324 293 189 196 237 227 67 336 ...\n```\n\n\n:::\n:::\n\n:::\n:::\n\n::: {.column width=20%}\n$\\bar X = 279.0$\n:::\n\n::: {.column width=50%}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-10-1.png){width=100%}\n:::\n:::\n\n:::\n\n:::\n\n\n\n::: {.columns}\n\n::: {.column width=30%}\n::: {.nudge-down}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n231 334 166 99 319 356 280 324 392 337 199 ...\n```\n\n\n:::\n:::\n\n:::\n:::\n\n::: {.column width=20%}\n$\\bar X = 285.5$\n:::\n\n::: {.column width=50%}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-12-1.png){width=100%}\n:::\n:::\n\n:::\n\n:::\n\n\n\n::: {.columns}\n\n::: {.column width=30%}\n::: {.nudge-down}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n183 202 156 465 349 441 368 219 199 387 306 ...\n```\n\n\n:::\n:::\n\n:::\n:::\n\n::: {.column width=20%}\n$\\bar X = 280.7$\n:::\n\n::: {.column width=50%}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-14-1.png){width=100%}\n:::\n:::\n\n:::\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n<!-- ## Effect of parameterisation -->\n\n<!-- ```{=html} -->\n<!-- <div id=\"desmos-normal\" style=\"position: relative; width: 100%; height: 88%;\"></div> -->\n\n<!-- <script> -->\n<!--   var elt = document.getElementById(\"desmos-normal\"); -->\n<!--   var calculator = Desmos.GraphingCalculator(elt, { -->\n<!--     expressions: true, -->\n<!--     settingsMenu: false, -->\n<!--     zoomButtons: true -->\n<!--   }); -->\n\n<!--   calculator.setExpressions([ -->\n<!--     { id: 'mu', latex: '\\\\mu = 0', sliderBounds: { min: -5, max: 5 } }, -->\n<!--     { id: 'sigma', latex: '\\\\sigma = 1', sliderBounds: { min: 0.5, max: 3 } }, -->\n<!--     { id: 'f', latex: 'f(x) = \\\\frac{1}{\\\\sqrt{2\\\\pi\\\\sigma^2}} e^{-\\\\frac{(x - \\\\mu)^2}{2\\\\sigma^2}}' }, -->\n<!--     { id: 'graph', latex: 'y = f(x)', color: Desmos.Colors.BLUE } -->\n<!--   ]); -->\n\n<!--   calculator.setMathBounds({ -->\n<!--     left: -8, -->\n<!--     right: 8, -->\n<!--     bottom: -0.05, -->\n<!--     top: 0.5 -->\n<!--   }); -->\n<!-- </script> -->\n<!-- ``` -->\n\n\n\n## Averaging (hypothetical) likelihoods\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/fishersamples-1.png){width=100%}\n:::\n:::\n\n\n\n## Fisher information \n\n::: {#def-fisher-info}\n#### Fisher information\nUnder certain regularity conditions, the Fisher information is defined as\n$$\n\\mathcal I(\\theta) = -\\operatorname{E}\\left[\\frac{d^2}{d\\theta^2} \\ell(\\theta)\\right].\n$$\n:::\n\nSo, it seems the Fisher information is **inversely** proportional to the *curvature of the (log)-likelihood function*.\n\n> üß† INTUITION: Large curvature => Harder to find the optimum => Less information about the parameter => More uncertainty!\n\nExtension of the concepts of \"curvature\" to the case of **random outcomes**!\n\n## Example (Normal mean Fisher information)\n\nFor the normal mean example, we have $\\ell'(\\mu) = \\frac{1}{\\sigma^2}\\sum_{i=1}^n (X_i-\\mu)$. Thus,\n$$\n\\begin{aligned}\n\\ell''(\\mu) = \\frac{d^2\\ell}{d\\mu^2}(\\mu) \n&= \\frac{d}{d\\mu} \\left[ \\frac{1}{\\sigma^2}\\sum_{i=1}^n (X_i-\\mu) \\right] \\\\\n&= \\frac{1}{\\sigma^2} \\sum_{i=1}^n (-1) = -n/\\sigma^2.\n\\end{aligned}\n$$\nTherefore, the Fisher information is\n$$\n\\mathcal I(\\mu) = -\\operatorname{E}\\left[\\ell''(\\mu)\\right] = \\frac{n}{\\sigma^2}.\n$$\nWe can improve the estimate of $\\mu$ by increasing the sample size $n$!\n\n## Large information ($n=1000$)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-15-1.png){width=100%}\n:::\n:::\n\n\n# Another example {.transition-slide-ubdrubydeep}\n\n## Exponential waiting time\n\n::: {#exm-exponential }\n\n#### Estimating failure rate of a machine component\n\nSuppose we collect data on how long (in hours) a machine component lasts before it fails. This could be a valve in a chemical plant, a sensor in a civil engineering structure, or a server part in a data centre.\n:::\n\n::: {.fragment fragment-index=1 .fade-out}\n![](figures/cogs.gif){.absolute left=300 width=\"40%\"}\n:::\n\n::: {.fragment fragment-index=1 .fade-in}\n::: {.nudge-up-medium}\nAssume that failure times $X$ follow an exponential distribution:\n$$\nf(x \\mid \\lambda) = \\lambda e^{-\\lambda x}, \\quad x > 0\n$$\nwhere $\\lambda$ is the failure **rate**. Using observed failure times from a sample of machines, we can estimate $\\lambda$ via **Maximum Likelihood Estimation (MLE)**.\n\nEngineers and analysts can predict average lifetime ($1/\\lambda$), schedule maintenance, and make design decisions to improve reliability.\n:::\n:::\n\n## Exponential waiting time (cont.)\n\nLet $X_1,\\dots,X_n$ be the observed failure times.\nThe log-likelihood function is given by\n$$\n\\begin{aligned}\n\\ell(\\lambda) \n= \\log \\left[ \\prod_{i=1}^n f(X_i \\mid \\lambda) \\right]\n&= \\sum_{i=1}^n \\log f(X_i \\mid \\lambda) \\\\\n&= \\sum_{i=1}^n \\log \\left( \\lambda e^{-\\lambda X_i} \\right) \\\\\n&= n \\log \\lambda - \\lambda \\sum_{i=1}^n X_i.\n\\end{aligned}\n$$\n\n## Exponential waiting time (cont.)\n\n\nTo find the MLE of $\\lambda$, we differentiate the log-likelihood function with respect to $\\lambda$ and set it to zero:\n$$\n\\begin{aligned}\n\\frac{d}{d\\lambda} \\ell(\\lambda)\n&= \\frac{n}{\\lambda} - \\sum_{i=1}^n X_i = 0 \\\\\n\\Leftrightarrow \\lambda &= \\frac{n}{\\sum_{i=1}^n X_i} = \\frac{1}{\\bar X}.\n\\end{aligned}\n$$\n\n::: {.callout-tip}\nTo obtain the Fisher information, just differentiate $\\ell'(\\lambda)$ once more, and take negative expectations.\nVerify that it is $\\mathcal I(\\lambda) = n/\\lambda^2$.\n:::\n\n## Data example\n\n## Plot\n\n# Conclusions {.transition-slide-ubdrubydeep}\n\n## Summary\n\n- Learn from data\n- Involves calculus!\n- Calculus is not just background math‚Äîit‚Äôs the **engine** of statistical theory.\n- Beyond undergraduate stats: everything from MLE, Bayesian posteriors, to machine learning involves gradients and Hessians.\n Suggest: for students who love math but unsure about statistics‚Äîyou are the kind of person statistics needs.\n\n\n\n## Probability vs statistics\n\n\nGiven a model, probability allows us to *predict* data.\nStatistics on the other hand, allows us to *learn* from data.\n\n\n::: {.columns}\n\n::: {.column width=48%}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n- What is $\\Pr(X > a)$?\n- What is $\\operatorname{E}(X)$?\n\n\n:::\n\n::: {.column width=48%}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n- What is $\\theta$?\n- Is $\\theta$ larger than $\\theta_0$?\n- How confident am I that $\\theta \\in (\\theta_l,\\theta_u)$?\n\n:::\n\n:::\n\n## Where to go from here\n\n- Numerical derivatives and specialised software\n- Bimodal distributions (not easy)\n- Interesting challenges in modern statistics include big data, high-dimensional data, and complex models.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
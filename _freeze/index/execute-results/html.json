{
  "hash": "b14713c24e75d45aa48f59acb74b7c2d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Calculus: Differentiation and Its Application'\nsubtitle: A statistical perspective\nformat:\n  ubd40-revealjs:\n    slide-level: 2\n    transition: fade\n    auto-stretch: false\n    width: 1250  # 1050\n    height: 760  # 700\n    self-contained: false\n    chalkboard: true\n    toc: false\n    toc-depth: 1\n    multiplex: true\n    code-block-height: 700px\n    # html-table-processing: none\nauthor:\n  - name: Dr. Haziq Jamil\n    orcid: 0000-0003-3298-1010\n    # email: haziq.jamil@ubd.edu.bn | `https://haziqj.ml/aiti-talk`\n    affiliations: \n      - 'Assistant Professor in Statistics, Universiti Brunei Darussalam'\n      - '<span style=\"font-style:normal;\">[`https://haziqj.ml/uitm-calculus/`](https://haziqj.ml/uitm-calculus/)</span>'\ndate: 2025-06-14\n# bibliography: refs.bib\nexecute:\n  echo: false\n  freeze: auto\n  cache: true\n---\n\n## (Almost) Everything you ought to know...\n\n### ...about calculus in the first year\n\n\n\nLet $f:\\mathcal X \\to \\mathbb R$ be a *real-valued* function defined on an input set $\\mathcal X$.\n\n::: {.fragment}\n\n::: {#def-derivatives}\n\n#### Differentiability\n\n$f(x)$ is said to be *differentiable* at a point $x \\in \\mathcal X$ if the limit \n\n$$\nL = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\n$${#eq-def-derivative}\nexists.\nIf $L$ exists, we denote it by $f'(x)$ or $\\frac{df}{dx}(x)$, and call it the *derivative* of $f$ at $x$.\nFurther, $f$ is said to be differentiable on $\\mathcal X$ if it is differentiable at every point in $\\mathcal X$.\n:::\n\n:::\n\n::: {.fragment}\nFor now, we assume $\\mathcal X \\subseteq \\mathbb R$, and will extend to higher dimensions later.\n:::\n\n## Some examples\n\n\n::: {.columns}\n\n::: {.column width=\"50%\"}\n<br>\n\n| Function | Derivative |\n| --- | --- |\n| $f(x) = x^2$ | $f'(x) = 2x$ |\n| $f(x) = \\sum_{n} a_n x^n$ | $f'(x) = \\sum_{n} n a_n x^{n-1}$ | \n| $f(x) = \\sin(x)$ | $f'(x) = \\cos(x)$ |\n| $f(x) = \\cos(x)$ | $f'(x) = -\\sin(x)$ |\n| $f(x) = e^x$ | $f'(x) = e^x$ |\n| $f(x) = \\ln(x)$ | $f'(x) = \\frac{1}{x}$ | \n| | |\n: {.table-ubd}\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"45%\"}\n::: {.fragment}\n::: {.nudge-down}\nWe can derive it \"by hand\" using the definition.\nLet $f(x) = x^2$.\nThen,\n<br>\n\n$$\n\\begin{align}\n\\lim_{h \\to 0} & \\frac{f(x + h) - f(x)}{h} \\\\\n&= \\lim_{h \\to 0} \\frac{(x + h)^2 - x^2}{h} \\\\\n&= \\lim_{h \\to 0} \\frac{x^2 + 2xh + h^2 - x^2}{h} \\\\[0.5em]\n&= \\lim_{h \\to 0} 2x + h \\\\[0.5em]\n&= 2x.\n\\end{align}\n$$\n:::\n:::\n:::\n\n:::\n\n::: aside\n[Taylor series](https://en.wikipedia.org/wiki/Taylor_series) is a powerful tool for approximating functions using derivatives. \n:::\n\n\n## Graphically...\n\n```{=html}\n<script src=\"https://www.desmos.com/api/v1.11/calculator.js?apiKey=dcb31709b452b1cf9dc26972add0fda6\"></script>\n\n<div id=\"desmos-derivative\" style=\"position: relative; width: 100%; height:88%;\"></div>\n\n<script>\n  var elt = document.getElementById(\"desmos-derivative\");\n  var calculator = Desmos.GraphingCalculator(elt, {\n    expressions: true,\n    settingsMenu: false,\n    zoomButtons: true\n  });\n\n  calculator.setExpressions([\n    { id: 'f', latex: 'f(x) = x^2' },\n    { id: 'a', latex: 'a = 1', sliderBounds: { min: -1, max: 3 } },\n    { id: 'h', latex: 'h = 1.5', sliderBounds: { min: 0.001, max: 1.5 } },\n    { id: 'pt1', latex: '(a, f(a))', showLabel: true },\n    { id: 'pt2', latex: '(a + h, f(a + h))', showLabel: true },\n    { id: 's', latex: 'L = \\\\frac{f(a+h) - f(a)}{h}' },\n    { id: 'tangent', latex: 'y = L(x - a) + f(a)', color: Desmos.Colors.RED }\n  ]);\n\n  calculator.setMathBounds({\n    left: -1,\n    right: 3,\n    bottom: -0.5,\n    top: 8\n  });\n\n</script>\n```\n\n## But *is* a derivative?\n\nThe derivative of a function tells you:    \n\n- üöÄ How fast the function is _changing_ at any point  \n- üìê The **slope** of the tangent line at that point\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-1-1.png){width=100%}\n:::\n:::\n\n\n## The concept of optimisation\n\n- When $f$ is some kind of a \"reward\" function, then the value of $x$ that maximises $f$ is highly of interest. Some examples:\n  - üí∞ **Profit maximisation**: Find the price that maximises profit.\n  - üß¨ **Biological processes**: Find the conditions that maximise growth or reproduction rates.\n  - üë∑‚Äç‚ôÇÔ∏è **Engineering**: Find the design parameters that maximise strength or efficiency.\n\n\n- Derivatives help us find so-called *critical values*: [Solve $f'(x) = 0$]{.ubdrubysoftbg}.\n\n::: {#exm-maxima}\n\nFind the maximum of $f(x) = -3x^4 + 4x^3 + 12x^2$.\n\n:::\n\n. . . \n\n$$\n\\begin{align*}\nf'(x) = -12x^3 + 12x^2 + 24x &= 0 \\\\\n\\Leftrightarrow 12x(2 + x - x^2) &= 0 \\\\\n\\Leftrightarrow 12x(x+1)(x-2) &= 0 \\\\\n\\Leftrightarrow x &= 0, -1, 2.\n\\end{align*}\n$$\n\n. . . \n\n::: {.nudge-up-medium}\nAre all of these critical values maxima values? ü§î\n:::\n\n## Graphically...\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-2-1.png){width=100%}\n:::\n:::\n\n\n\n## How do we know if it's a maxima or minima?\n\n[**Second derivative test:**]{.ubdrubydeep} Measure the **change in slope** around the critical point $\\hat x$, i.e. $f''(\\hat x) = \\frac{d}{dx}\\left( \\frac{df}{dx}(x) \\right) = \\frac{d^2f}{dx^2}(x)$.\n\n. . . \n\n::: {.nudge-down-medium}\n\n| Behaviour of $f$ near $\\hat x$               | $f''(\\hat x)$       | Shape             | Conclusion     |\n|-------------------------------|---------------------|----------------------|-------------------|----------------|\n| Increasing ‚Üí Decreasing       | $f''(\\hat x) < 0$ | Concave (‚à©)  | Local maximum  |\n| Decreasing ‚Üí Increasing       | $f''(\\hat x) > 0$ | Convex (‚à™)    | Local minimum  |\n| No sign change / flat region  |  $f''(\\hat x) = 0$   | Unknown / flat    | Inconclusive   |\n\n:::\n\n## Second derivative test\n\nFrom Example 1, the second derivative is given by\n$$\n\\begin{align*}\nf''(x) &= \\frac{d}{dx}\\left(-12x^3 + 12x^2 +24x\\right) \\\\\n&= -36x^2 + 24x + 24\n\\end{align*}\n$$\n\n::: {.nudge-up}\nPlug in the critical points:\n\n- $x=-1$: $f''(-1) = -36 - 24 + 24 = -36 < 0$, hence local maximum.\n- $x=0$: $f''(0) = 0 + 0 + 24 = 24 > 0$, hence local minimum.\n- $x=2$: $f''(2) = -144 + 48 + 24 = -72 < 0$, hence local maximum.\n\n::: {.callout-tip}\nOften it is not enough to just differentiate once to find optima. You may need to differentiate twice to classify the critical points.\n:::\n\n:::\n\n## Curvature\n\nLet $\\mathcal C_x$ denote the *osculating circle* at $x$ with centre $c$ and radius $r$, i.e. the circle that best approximates the graph of $f$ at $x$.\nHistorically, the curvature $\\kappa$ for a graph of a function $f$ at a point $x$ is measured as $\\kappa = \\frac{1}{r}$.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/curvature-concav-1.png){width=100%}\n:::\n:::\n\n\n## Curvature and concavity\n\n::: {#def-curvature-alt}\n#### Curvature\nThe (signed) curvature for a graph $y=f(x)$ is\n$$\n\\kappa = \\frac{f''(x)}{\\big(1 + [f'(x)]^2\\big)^{3/2}}.\n$$\n:::\n\n- The second derivative $f''(x)$ tells us how fast the slope is changing.\n\n- The [**sign**]{.ubdrubydeep} of the curvature is the same as the sign of $f''(x)$. Hence,\n  - If $f''(x) > 0$, the graph is **concave up** (convex).\n  - If $f''(x) < 0$, the graph is **concave down** (concave).\n\n- The [**magnitude**]{.ubdrubydeep} of the curvature is proportional to $f''(x)$. Hence,\n  - If $|f''(x)|$ is large, the graph is *steep* and \"curvier\".\n  - If $|f''(x)|$ is small, the graph is *flat* and \"gentle\".\n  \n- For reference, a straight line has zero curvature.\n\n## Summary so far\n\n- Derivatives represent rate of change (slope) of a function $f:\\mathcal X \\to \\mathbb R$.\n\n- Interested in optimising an *objective* function $f(x)$ representing some kind of \"reward\" or \"cost\".\n\n- Find critical points by solving $f'(x) = 0$.\n\n- Use the second derivative test to classify critical points:\n  - If $f''(x) < 0$, then $f$ is concave down at $x$ and $x$ is a local maximum.\n  - If $f''(x) > 0$, then $f$ is concave up at $x$ and $x$ is a local minimum.\n  - If $f''(x) = 0$, then the test is inconclusive.\n  \n- Curvature tells us how steep the curve is at its optima. In some sense, it tells us how hard or easy it is to find the optimum.\n\n# A statistical persepctive {.transition-slide-ubdrubydeep}\n\n## But what *is* statistics?\n\nStatistics is a scientific subject that deals with the collection, analysis, interpretation, and presentation of data.\n\n- **Collection** means designing experiments, questionnaires, sampling schemes, and also administration of data collection.\n\n- **Analysis** means mathematically modelling, estimation, testing, forecasting.\n\n::: {.nudge-up}\n![](figures/ppdac.png){width=100% fig-align=center}\n:::\n\n::: aside\nSee also: *The Art of Statistics: Learning from Data* by David Spiegelhalter.\n:::\n\n## Motivation\n\n> I toss a coin $n$ times and I wish to find $p$, the probability of heads. Let $X_i=1$ if a heads turns up, and $X_i=0$ if tails.\n\n- I do not know the value of $p$, so I want to estimate it somehow.\n\n- I have a \"guess\" what it might be e.g. $p=0.5$ or $p=0.7$.\n\n- How do I objectively decide which value is better?\n\n![](figures/bentcoin.png){.absolute right=10 top=200 width=\"220\"}\n\n\n\n. . . \n\n::: {.nudge-up}\n::: {.callout-note}\n#### A more high-stakes example\n\nThink of the binary outcomes as a stock price rising or falling. \nYou'll need to decide to invest based on what you believe (or the data suggests) the probability of the stock price rising is.\n:::\n:::\n\n## A probabilistic model\n\nEach $X_i$ is a *random variable* taking only two possible outcomes, i.e.\n$$\nX_i = \\begin{cases}\n1 &\\text{w.p. } \\ \\ p \\\\\n0 &\\text{w.p. } \\ \\ 1-p \\\\\n\\end{cases}\n$$\nThis is known as a **Bernoulli** random variable.\n\n. . .\n\nSuppose that $X=X_1 + \\dots + X_n$. So we are counting the number of heads in $n$ tossess. Then this becomes a **binomial** random variable. We write $X \\sim \\operatorname{Bin}(n, p)$, and the *probability mass function* is given by\n$$\nf(x \\mid p) = \\Pr(X = x) = \\binom{n}{x} p^x (1 - p)^{n - x}, \\quad x = 0, 1, \\ldots, n. \n$$\n\n::: {.nudge-up-medium}\n[Often we might want to find quantities such as $\\operatorname{E}(X)=np$ and $\\operatorname{Var}(X)=np(1-p)$, but we will not go into details here.]{.ubdgrey}\n:::\n\n## Learning from data\n\n::: {.callout-important}\n#### $p$ is unknown\nIf we do not know $p$, then it is not possible to calculate probabilities, expectations, variances... ‚òπÔ∏è\n:::\n\n. . .\n\nNaturally, you go ahead and collect data by tossing it $n=10$ times. The outcome happens to be\n$$\nH, H, H, T, T, H, H, T, H, H\n$$\nThere is a total of $X=7$ heads, and from this you surmise that (at least) the coin is *unlikely* to be fair, because:\n\n::: {.nudge-down-small}\n\n- If $p=0.5$, then $\\Pr(X=7 \\mid p = 0.7) = \\binom{10}{7} (0.5)^7 (0.5)^3 = 0.117$.\n- If $p=0.7$, then $\\Pr(X=7 \\mid p = 0.5) = \\binom{10}{7} (0.7)^7 (0.3)^3 = 0.267$.\n- If $p=0.9$, then $\\Pr(X=7 \\mid p = 0.9) = \\binom{10}{7} (0.9)^7 (0.1)^3 = 0.057$.\n\n:::\n\n\n::: {.nudge-up-medium}\nHow to formalise this idea?\n:::\n\n## The likelihood function\n\n::: {#def-likelihood}\nGiven a probability function $x \\mapsto f(x\\mid\\theta)$ where $x$ is a realisation of a random variable $X$, the *likelihood function* is $\\theta \\mapsto f(x\\mid\\theta)$, often written\n$\\mathcal L(\\theta) = f(x \\mid \\theta)$.\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-3-1.png){width=100%}\n:::\n:::\n\n\n\n## Parameteric statistical models\n\nAssume that $X_i \\sim f(x \\mid \\theta)$ independently for $i = 1, \\ldots, n$.\nHere, functional form of $f$ is known, but the parameter $\\theta$ is unknown. Examples:\n\n| Name        | $f(x \\mid \\theta)$                                                   | $\\theta$                                | Remarks                          |\n|-------------|----------------------------------------------------------------------|------------------------------------------|----------------------------------|\n| Binomial    | $\\binom{n}{x} p^x (1 - p)^{n - x}$                                   | $p \\in (0, 1)$        | No. successes in $n$ trials        |\n| Poisson     | $\\frac{\\lambda^x e^{-\\lambda}}{x!}$                                  | $\\lambda > 0$                            | Count data  |\n| Uniform     | $\\frac{1}{b - a}$ for $x \\in [a, b]$                                 | $a < b$                                  | Equally likely outcomes     |\n| Exponential | $\\lambda e^{-\\lambda x}$ for $x \\geq 0$                              | $\\lambda > 0$                            | Waiting time        |\n| Normal      | $\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$ | $\\mu \\in \\mathbb{R},\\ \\sigma^2 > 0$ | Bell curve       |\n|   |   |   |\n: {.table-ubd}\n\n\n## Example (Normal mean)\n\nSuppose we observe $X_1, \\ldots, X_n$ from a normal distribution with unknown mean $\\mu$ and known variance $\\sigma^2$.\nThe log-likelihood function is given by\n\n. . .\n\n$$\n\\begin{align*}\n\\ell(\\mu) &= \\sum_{i=1}^n \\log f(X_i \\mid \\mu) \\\\\n&= \\sum_{i=1}^n \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(X_i - \\mu)^2}{2\\sigma^2}} \\right) \\\\\n&= \\sum_{i=1}^n \\left\\{ -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(X_i - \\mu)^2}{2\\sigma^2} \\right\\} \\\\\n&= -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (X_i - \\mu)^2.\n\\end{align*}\n$$\n\n## Example (Normal mean, cont.)\n\nTo find the MLE of $\\mu$, we differentiate the log-likelihood function with respect to $\\mu$ and set it to zero:\n\n. . .\n\n$$\n\\begin{align*}\n\\frac{d}{d\\mu} \\ell(\\mu) &= -\\frac{1}{2\\sigma^2} \\cdot 2 \\sum_{i=1}^n (X_i - \\mu)(-1) \\\\\n&= \\frac{1}{\\sigma^2} \\sum_{i=1}^n (X_i - \\mu) = 0 \\\\\n&\\Leftrightarrow \\sum_{i=1}^n X_i - n\\mu = 0 \n\\Leftrightarrow \\mu = \\frac{1}{n} \\sum_{i=1}^n X_i.\n\\end{align*}\n$$\n\nThus, the MLE for $\\mu$ is $\\hat\\mu = \\bar X = \\frac{1}{n} \\sum_{i=1}^n X_i$.\n\n## A real data example\n\n::: {#exm-house-price}\nSample $n=50$ house prices randomly in Brunei.\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-4-1.png){width=100%}\n:::\n:::\n\n\n\n::: aside\nHJ (2025). A spatio-temporal analysis of house prices in Brunei Darussalam. *Qual Quant*, 1-32. DOI: [10.1007/s11135-025-02164-0](https://doi.org/10.1007/s11135-025-02164-0).\n:::\n\n\n## But wait, the sample was random...\n\n::: {.columns}\n\n::: {.column width=30%}\n::: {.nudge-down}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n209 290 188 432 305 190 321 346 330 241 423 ...\n```\n\n\n:::\n:::\n\n:::\n:::\n\n::: {.column width=20%}\n$\\bar X = 282.0$\n:::\n\n::: {.column width=50%}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-6-1.png){width=100%}\n:::\n:::\n\n:::\n\n:::\n\n\n\n::: {.columns}\n\n::: {.column width=30%}\n::: {.nudge-down}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n312 211 306 159 415 470 235 168 329 258 512 ...\n```\n\n\n:::\n:::\n\n:::\n:::\n\n::: {.column width=20%}\n$\\bar X = 283.7$\n:::\n\n::: {.column width=50%}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-8-1.png){width=100%}\n:::\n:::\n\n:::\n\n:::\n\n\n::: {.columns}\n\n::: {.column width=30%}\n::: {.nudge-down}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n210 276 181 288 207 449 344 363 310 440 208 ...\n```\n\n\n:::\n:::\n\n:::\n:::\n\n::: {.column width=20%}\n$\\bar X = 256.8$\n:::\n\n::: {.column width=50%}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-10-1.png){width=100%}\n:::\n:::\n\n:::\n\n:::\n\n\n\n::: {.columns}\n\n::: {.column width=30%}\n::: {.nudge-down}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n317 270 240 179 123 164 372 210 134 459 315 ...\n```\n\n\n:::\n:::\n\n:::\n:::\n\n::: {.column width=20%}\n$\\bar X = 279.7$\n:::\n\n::: {.column width=50%}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-12-1.png){width=100%}\n:::\n:::\n\n:::\n\n:::\n\n\n\n::: {.columns}\n\n::: {.column width=30%}\n::: {.nudge-down}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n313 441 431 239 43 522 339 326 271 323 256 ...\n```\n\n\n:::\n:::\n\n:::\n:::\n\n::: {.column width=20%}\n$\\bar X = 268.9$\n:::\n\n::: {.column width=50%}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-14-1.png){width=100%}\n:::\n:::\n\n:::\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n<!-- ## Effect of parameterisation -->\n\n<!-- ```{=html} -->\n<!-- <div id=\"desmos-normal\" style=\"position: relative; width: 100%; height: 88%;\"></div> -->\n\n<!-- <script> -->\n<!--   var elt = document.getElementById(\"desmos-normal\"); -->\n<!--   var calculator = Desmos.GraphingCalculator(elt, { -->\n<!--     expressions: true, -->\n<!--     settingsMenu: false, -->\n<!--     zoomButtons: true -->\n<!--   }); -->\n\n<!--   calculator.setExpressions([ -->\n<!--     { id: 'mu', latex: '\\\\mu = 0', sliderBounds: { min: -5, max: 5 } }, -->\n<!--     { id: 'sigma', latex: '\\\\sigma = 1', sliderBounds: { min: 0.5, max: 3 } }, -->\n<!--     { id: 'f', latex: 'f(x) = \\\\frac{1}{\\\\sqrt{2\\\\pi\\\\sigma^2}} e^{-\\\\frac{(x - \\\\mu)^2}{2\\\\sigma^2}}' }, -->\n<!--     { id: 'graph', latex: 'y = f(x)', color: Desmos.Colors.BLUE } -->\n<!--   ]); -->\n\n<!--   calculator.setMathBounds({ -->\n<!--     left: -8, -->\n<!--     right: 8, -->\n<!--     bottom: -0.05, -->\n<!--     top: 0.5 -->\n<!--   }); -->\n<!-- </script> -->\n<!-- ``` -->\n\n\n\n## Averaging (hypothetical) likelihoods\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/fishersamples-1.png){width=100%}\n:::\n:::\n\n\n## Averaging (hypothetical) likelihoods, cont.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-15-1.png){width=100%}\n:::\n:::\n\n\n## Fisher information \n\n::: {#def-fisher-info}\n#### Fisher information\nUnder certain regularity conditions, the Fisher information is defined as\n$$\n\\mathcal I(\\theta) = -\\operatorname{E}\\left[\\frac{d^2}{d\\theta^2} \\ell(\\theta)\\right].\n$$\n:::\n\n. . .\n\nEvidently the Fisher information is **proportional** to the *curvature of the (log)-likelihood function*.\n\n> üß† INTUITION: Stronger curvature ‚Üí Easier to find optima ‚Üí More information about the parameter ‚Üí Less uncertainty (and vice versa)\n\nExtension of the concepts of \"curvature\" to the case of **random outcomes**!\n\n## Example (Normal mean Fisher information)\n\nFor the normal mean example, we have $\\ell'(\\mu) = \\frac{1}{\\sigma^2}\\sum_{i=1}^n (X_i-\\mu)$. Thus,\n\n. . . \n\n$$\n\\begin{aligned}\n\\ell''(\\mu) = \\frac{d^2\\ell}{d\\mu^2}(\\mu) \n&= \\frac{d}{d\\mu} \\left[ \\frac{1}{\\sigma^2}\\sum_{i=1}^n (X_i-\\mu) \\right] \\\\\n&= \\frac{1}{\\sigma^2} \\sum_{i=1}^n (-1) = -n/\\sigma^2.\n\\end{aligned}\n$$\nTherefore, the Fisher information is\n$$\n\\mathcal I(\\mu) = -\\operatorname{E}\\left[\\ell''(\\mu)\\right] = \\frac{n}{\\sigma^2}.\n$$\nWe can improve the estimate of $\\mu$ by increasing the sample size $n$!\n\n## Large information ($n=1000$)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-16-1.png){width=100%}\n:::\n:::\n\n\n# Another example {.transition-slide-ubdrubydeep}\n\n## Exponential waiting time\n\n::: {#exm-exponential }\n\n#### Estimating failure rate of a machine component\n\nSuppose we collect data on how long (in hours) a machine component lasts before it fails. This could be a valve in a chemical plant, a sensor in a civil engineering structure, or a server part in a data centre.\n:::\n\n::: {.fragment fragment-index=1 .fade-out}\n![](figures/cogs.gif){.absolute left=330 width=\"40%\"}\n:::\n\n::: {.fragment fragment-index=1 .fade-in}\n::: {.nudge-up-medium}\nAssume that failure times $X$ follow an exponential distribution:\n$$\nf(x \\mid \\lambda) = \\lambda e^{-\\lambda x}, \\quad x > 0\n$$\nwhere $\\lambda$ is the failure **rate**. Using observed failure times from a sample of machines, we can estimate $\\lambda$ via **Maximum Likelihood Estimation (MLE)**.\n\nEngineers and analysts can predict average lifetime ($1/\\lambda$), schedule maintenance, and make design decisions to improve reliability.\n:::\n:::\n\n## Exponential waiting time (cont.)\n\nLet $X_1,\\dots,X_n$ be the observed failure times.\nThe log-likelihood function is given by\n\n. . .\n\n$$\n\\begin{aligned}\n\\ell(\\lambda) \n= \\log \\left[ \\prod_{i=1}^n f(X_i \\mid \\lambda) \\right]\n&= \\sum_{i=1}^n \\log f(X_i \\mid \\lambda) \\\\\n&= \\sum_{i=1}^n \\log \\left( \\lambda e^{-\\lambda X_i} \\right) \\\\\n&= n \\log \\lambda - \\lambda \\sum_{i=1}^n X_i.\n\\end{aligned}\n$$\n\n## Exponential waiting time (cont.)\n\n\nTo find the MLE of $\\lambda$, we differentiate the log-likelihood function with respect to $\\lambda$ and set it to zero:\n$$\n\\begin{aligned}\n\\frac{d}{d\\lambda} \\ell(\\lambda)\n&= \\frac{n}{\\lambda} - \\sum_{i=1}^n X_i = 0 \\\\\n\\Leftrightarrow \\lambda &= \\frac{n}{\\sum_{i=1}^n X_i} = \\frac{1}{\\bar X}.\n\\end{aligned}\n$$\n\n::: {.callout-tip}\nTo obtain the Fisher information, just differentiate $\\ell'(\\lambda)$ once more, and take negative expectations.\nVerify that it is $\\mathcal I(\\lambda) = n/\\lambda^2$.\n:::\n\n## Data example\n\nSuppose $n=50$ machines were observed, and the failure times (in hours) recorded:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n 293.4  339.4  392.6   84.4   36.9  792.5   88.8  844.1  182.6  103.7 \n 364.5   73.7  578.6  101.9  143.9  459.4  200.2  206.0  461.5  301.2 \n 199.6  218.0   76.5   89.8  324.3  240.5 2022.2  264.8  213.8  901.3 \n 219.9  729.4 1322.9  551.3  571.2  428.1  781.1  395.7   18.2   50.9 \n 322.6  110.6  157.4  310.5  477.7  168.4    9.2  969.1  399.5    5.4 \n```\n\n\n:::\n:::\n\n\nSince the MLE of $\\lambda$ is $\\hat\\lambda = 1/\\bar X$, we can compute it as follows:\n$$\n\\hat\\lambda = \\frac{1}{\\bar X} = \\frac{1}{372.0} \\approx 2.69e-03.\n$$\nIn other words, approximately one failure every 372 hours.\n\n## Plot of data and exponential fit\n\n```{=html}\n<div id=\"desmos-failure\" style=\"position: relative; width: 100%; height:88%;\"></div>\n\n<script>\n  var elt = document.getElementById(\"desmos-failure\");\n  var calculator = Desmos.GraphingCalculator(elt, {\n    expressions: true,\n    settingsMenu: false,\n    zoomButtons: true\n  });\n\n  const data = [\n    680.62, 416.51, 823.75, 20.37, 744.01, 553.79, 154.83, 550.31, 116.22, 403.03,\n    27.08, 175.62, 589.77, 634.45, 81.44, 37.15, 424.29, 60.09, 76.06, 138.36,\n    126.04, 16.06, 390.87, 187.40, 540.41, 369.30, 320.58, 14.52, 791.19, 436.69,\n    380.14, 767.88, 202.87, 182.32, 62.08, 21.22, 115.61, 284.69, 193.08, 79.84,\n    0.36, 503.10, 148.72, 407.79, 76.35, 815.07, 191.84, 70.73, 11.55, 384.71\n  ];\n\n  calculator.setExpressions([\n    {\n      id: \"xbar_slider\",\n      latex: \"m=300\",\n      sliderBounds: { min: 50, max: 900, step: 2 }\n    },\n    // Set the lambda slider\n    {\n      id: \"lambda_def\",\n      latex: \"\\\\lambda = \\\\frac{1}{m}\"\n    },\n\n    // Define the exponential function\n    {\n      id: \"exp_fn\",\n      latex: \"f(x)=\\\\lambda e^{-\\\\lambda x}\",\n      color: Desmos.Colors.RED\n    },\n\n    // Input data as a list\n    {\n      id: \"data_vec\",\n      latex: \"X=[\" + data.join(\",\") + \"]\",\n      color: Desmos.Colors.BLUE\n    },\n    // Plot the histogram of the data\n    {\n      id: \"hist\",\n      latex: \"\\\\histogram(X,150)\",\n      color: Desmos.Colors.GREEN,\n      hidden: true\n    }\n  ]);\n\n  calculator.setMathBounds({\n    left: 0,\n    right: 900,\n    bottom: 0,\n    top: 0.005\n  });\n</script>\n```\n\n# Conclusions {.transition-slide-ubdrubydeep}\n\n## Summary\n\n\nGiven a model, probability allows us to *predict* data.\nStatistics on the other hand, allows us to *learn* from data.\n\n\n::: {.columns}\n\n::: {.column width=48%}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n::: {.column width=48%}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n:::\n\n. . .\n\n- Calculus plays a central role:\n  - Finding the **Maximum Likelihood Estimator (MLE)**\n  - Understanding **Fisher Information** and curvature\n  - Uncovering role of sample size in estimation uncertainty\n- Calculus is not just background math‚Äîit‚Äôs the **engine** driving statistical theory.\n\n## Where to go from here\n\n::: {.columns}\n\n::: {.column width=50%}\nüåê **Numerical derivatives** ‚Äî how computers approximate calculus\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfun <- function(x) x ^ 2\nnumDeriv::grad(fun, x = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4\n```\n\n\n:::\n:::\n\n\n::: {.nudge-down-small}\nüöÄ Modern statistics tackles:\n\n\n- **Big data** (too much)\n- **High-dimensional data** (too many variables)\n- **Complex models** (real-world messiness)\n:::\n\n\n:::\n\n::: {.column width=50%}\nüß† **Bimodal and non-standard distributions**  when simple models break\n\n\n\n:::\n\n:::\n\n::: {.absolute right=-300 bottom=50}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/mleunique-1.png){width=70%}\n:::\n:::\n\n:::\n\n# Thanks! {.transition-slide-ubdrubydeep}\n\n[`https://haziqj.ml/`](https://haziqj.ml/)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
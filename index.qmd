---
title: 'Calculus: Differentiation and Its Application'
subtitle: A statistical perspective
format:
  ubd40-revealjs:
    slide-level: 2
    transition: fade
    auto-stretch: false
    width: 1250  # 1050
    height: 760  # 700
    self-contained: false
    chalkboard: true
    toc: false
    toc-depth: 1
    multiplex: true
    code-block-height: 700px
    # html-table-processing: none
author:
  - name: Dr. Haziq Jamil
    orcid: 0000-0003-3298-1010
    # email: haziq.jamil@ubd.edu.bn | `https://haziqj.ml/aiti-talk`
    affiliations: 
      - 'Assistant Professor in Statistics, Universiti Brunei Darussalam'
      - '<span style="font-style:normal;">[`https://haziqj.ml/uitm-calculus/`](https://haziqj.ml/uitm-calculus/)</span>'
date: 2025-06-14
# bibliography: refs.bib
execute:
  echo: false
  freeze: auto
  cache: false
---

## (Almost) Everything you ought to know...

### ...about calculus in the first year



Let $f:\mathcal X \to \mathbb R$ be a *real-valued* function defined on an input set $\mathcal X$.

::: {.fragment}

::: {#def-derivatives}

#### Differentiability

$f(x)$ is said to be *differentiable* at a point $x \in \mathcal X$ if the limit 

$$
L = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
$${#eq-def-derivative}
exists.
If $L$ exists, we denote it by $f'(x)$ or $\frac{df}{dx}(x)$, and call it the *derivative* of $f$ at $x$.
Further, $f$ is said to be differentiable on $\mathcal X$ if it is differentiable at every point in $\mathcal X$.
:::

:::

::: {.fragment}
For now, we assume $\mathcal X \subseteq \mathbb R$, and will extend to higher dimensions later.
:::

## Some examples


::: {.columns}

::: {.column width="50%"}
<br>

| Function | Derivative |
| --- | --- |
| $f(x) = x^2$ | $f'(x) = 2x$ |
| $f(x) = \sum_{n} a_n x^n$ | $f'(x) = \sum_{n} n a_n x^{n-1}$ | 
| $f(x) = \sin(x)$ | $f'(x) = \cos(x)$ |
| $f(x) = \cos(x)$ | $f'(x) = -\sin(x)$ |
| $f(x) = e^x$ | $f'(x) = e^x$ |
| $f(x) = \ln(x)$ | $f'(x) = \frac{1}{x}$ | 
| | |
: {.table-ubd}
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}

<br>

We can derive it "by hand" using the definition.
Let $f(x) = x^2$.
Then,

<br>
<br>

$$
\begin{align}
\lim_{h \to 0} & \frac{f(x + h) - f(x)}{h} \\
&= \lim_{h \to 0} \frac{(x + h)^2 - x^2}{h} \\
&= \lim_{h \to 0} \frac{x^2 + 2xh + h^2 - x^2}{h} \\[0.5em]
&= \lim_{h \to 0} 2x + h \\[0.5em]
&= 2x.
\end{align}
$$

:::

:::

::: aside
[Taylor series](https://en.wikipedia.org/wiki/Taylor_series) is a powerful tool for approximating functions using derivatives. 
:::


## Graphically...

```{=html}
<script src="https://www.desmos.com/api/v1.11/calculator.js?apiKey=dcb31709b452b1cf9dc26972add0fda6"></script>

<div id="desmos-derivative" style="position: relative; width: 100%; height:88%;"></div>

<script>
  var elt = document.getElementById("desmos-derivative");
  var calculator = Desmos.GraphingCalculator(elt, {
    expressions: true,
    settingsMenu: false,
    zoomButtons: true
  });

  calculator.setExpressions([
    { id: 'f', latex: 'f(x) = x^2' },
    { id: 'a', latex: 'a = 1', sliderBounds: { min: -1, max: 3 } },
    { id: 'h', latex: 'h = 1.5', sliderBounds: { min: 0.001, max: 1.5 } },
    { id: 'pt1', latex: '(a, f(a))', showLabel: true },
    { id: 'pt2', latex: '(a + h, f(a + h))', showLabel: true },
    { id: 's', latex: 'L = \\frac{f(a+h) - f(a)}{h}' },
    { id: 'tangent', latex: 'y = L(x - a) + f(a)', color: Desmos.Colors.RED }
  ]);

  calculator.setMathBounds({
    left: -1,
    right: 3,
    bottom: -0.5,
    top: 8
  });

</script>
```

## What is a derivative?

The derivative of a function tells you:    

- üöÄ How fast the function is _changing_ at any point  
- üìê The **slope** of the tangent line at that point

```{r}
#| fig-height: 2.8
#| fig-width: 7
#| out-width: 100%
library(tidyverse)

f <- function(x) -x^2
f_prime <- function(x) -2*x
x_pts <- c(-1, 0, 1)
fty <- factor(c("f increasing", "f stationary", "f decreasing"),
                levels = c("f increasing", "f stationary", "f decreasing"))

curve_df <- tibble(x = seq(-2, 2, length.out = 200), y = f(x))

tangent_df <- lapply(x_pts, function(a) {
  tibble(x = seq(a - 1, a + 1, length.out = 100),
         y = f_prime(a) * x + f(a) - f_prime(a) * a,
         x0 = a,
         type = fty[match(a, x_pts)])
}) |> bind_rows()

point_df <- tibble(x = x_pts, y = f(x_pts), type = fty)

ggplot() +
  geom_line(data = curve_df, aes(x, y), color = "black", linewidth = 0.8) +
  geom_line(data = tangent_df, aes(x, y), color = "red3", linewidth = 0.8) +
  geom_point(data = point_df, aes(x, y), color = "steelblue3", size = 2) +
  facet_wrap(~ type) +
  labs(x = "x", y = "f(x)") +
  theme_bw()
```

## The concept of optimisation

- When $f$ is some kind of a "reward" function, then the value of $x$ that maximises $f$ is highly of interest. Some examples:
  - üí∞ **Profit maximisation**: Find the price that maximises profit.
  - üß¨ **Biological processes**: Find the conditions that maximise growth or reproduction rates.
  - üë∑‚Äç‚ôÇÔ∏è **Engineering**: Find the design parameters that maximise strength or efficiency.


- Derivatives help us find so-called *critical values*: [Solve $f'(x) = 0$]{.ubdrubysoftbg}.

::: {#exm-maxima}

Find the maximum of $f(x) = -3x^4 + 4x^3 + 12x^2$.

:::

. . . 

$$
\begin{align*}
f'(x) = -12x^3 + 12x^2 + 24x &= 0 \\
\Leftrightarrow 12x(2 + x - x^2) &= 0 \\
\Leftrightarrow 12x(x+1)(x-2) &= 0 \\
\Leftrightarrow x &= 0, -1, 2.
\end{align*}
$$

. . . 

::: {.nudge-up-medium}
Are all of these critical values maxima values? ü§î
:::

## Graphically...

```{r}
#| fig-height: 4.1
#| fig-width: 7
#| out-width: 100%
library(ggrepel)

f <- function(x) -3 * x ^ 4 + 4 * x ^ 3 + 12 * x ^ 2#-x^4 + 2*x^3 + 3*x^2 - 2*x
x_crit <- c(-1, 0, 2)
df <- tibble(x = seq(-2, 3, length.out = 500)) %>%
  mutate(y = f(x))

critical_points <- tibble(
  x = x_crit,
  label = c("Local Max", "Local Min", "Global Max"),
  color = RColorBrewer::brewer.pal(3, "Set1")
) %>%
  mutate(y = f(x))

shade_df <- tibble(
  xmin = c(-Inf, x_crit[1], x_crit[2]),
  xmax = c(x_crit[1], x_crit[2], x_crit[3]),
  fill = rep(c("grey75", "white"), length.out = 3)
)

pm_df <- tibble(
  x = ((c(-2.2, x_crit, 3.2) + c(x_crit, 3.2, 0)) / 2)[1:4],
  y = 35
)

ggplot(df, aes(x, y)) +
  geom_rect(data = shade_df, aes(xmin = xmin, xmax = xmax, ymin = -Inf, 
                                 ymax = Inf, fill = fill), 
            inherit.aes = FALSE, alpha = 0.4) +  
  scale_fill_identity() +
  geom_label(data = pm_df, aes(x, y, label = c("+", "‚àí", "+", "‚àí")),
            size = 8, color = "black", vjust = -0.5) +
  geom_line(color = "black", size = 1) +
  geom_point(data = critical_points, aes(x, y, color = label), size = 3,
             show.legend = FALSE) +
  geom_text_repel(
    data = critical_points, 
    aes(x, y, label = label, color = label), 
    size = 4.5, 
    seed = 4, 
    #nudge_y = 0.5,
    box.padding = 1, 
    point.padding = 0.4, 
    show.legend = FALSE
  ) +
  scale_colour_brewer(palette = "Set1") +
  coord_cartesian(ylim = c(-8, 43)) +
  labs(x = "x", y = "f(x)") +
  theme_bw()
```


## How do we know if it's a maxima or minima?

[**Second derivative test:**]{.ubdrubydeep} Measure the **change in slope** around the critical point $\hat x$, i.e. $f''(\hat x) = \frac{d}{dx}\left( \frac{df}{dx}(x) \right) = \frac{d^2f}{dx^2}(x)$.

. . . 

::: {.nudge-down-medium}

| Behaviour of $f$ near $\hat x$               | $f''(\hat x)$       | Shape             | Conclusion     |
|-------------------------------|---------------------|----------------------|-------------------|----------------|
| Increasing ‚Üí Decreasing       | $f''(\hat x) < 0$ | Concave (‚à©)  | Local maximum  |
| Decreasing ‚Üí Increasing       | $f''(\hat x) > 0$ | Convex (‚à™)    | Local minimum  |
| No sign change / flat region  |  $f''(\hat x) = 0$   | Unknown / flat    | Inconclusive   |

:::

## Curvature

::: {#def-curvature}
Let $\mathcal C_x$ denote the *osculating circle* at $x$ with centre $c$ and radius $r$, i.e. the circle that best approximates the graph of $f$ at $x$.
The curvature $\kappa$ for a graph of a function $f$ at a point $x$ is defined as $\kappa = \frac{1}{r}$.
:::

```{r}
#| fig-width: 7
#| fig-height: 3
#| out-width: 100%
#| label: curvature-concav
set.seed(123)
n1 <- 5
n2 <- 500
nratio <- sqrt(n2 / n1)
m <- 100
B <- 200
mu <- c(seq(5.8, 10.2, length = m - 1), 8)
res <- matrix(NA, nrow = m, ncol = B) 
score_vec1 <- score_vec2 <- mean_vec2 <- mean_vec1 <- rep(NA, B)

# Small FI
for (j in 1:B) {
  X <- rnorm(n1, mean = 8, sd = 1)
  ll <- rep(NA, length(mu))
  for (i in seq_along(ll)) 
    ll[i] <- sum(dnorm(X, mean = mu[i], sd = 1, log = TRUE)) 
  res[, j] <- ll
  mean_vec1[j] <- mean(X)
  score_vec1[j] <- sum(X - 8)
}
colnames(res) <- 1:B
res1 <-
  as_tibble(nratio * res) %>%
  mutate(mu = mu,
         mean = apply(nratio * res, 1, mean),
         n = n1) 

# Big FI
for (j in 1:B) {
  X <- rnorm(n2, mean = 8, sd = 1)
  ll <- rep(NA, length(mu))
  for (i in seq_along(ll)) 
    ll[i] <- sum(dnorm(X, mean = mu[i], sd = 1, log = TRUE)) 
  res[, j] <- ll
  mean_vec2[j] <- mean(X)
  score_vec2[j] <- sum(X - 8)
}
colnames(res) <- 1:B
res2 <-
  as_tibble(res) %>%
  mutate(mu = mu,
         mean = apply(res, 1, mean),
         n = n2) 

res <- bind_rows(res1, res2) %>%
  pivot_longer(`1`:`B`)

tmp <- res %>% filter(mu == 8) %>% summarise(mean = unique(mean)) %>% unlist()
diff <- max(tmp) - min(tmp)

res <-
  res |>
  mutate(
    value = case_when(
      n == n2 ~ value + diff,
      TRUE ~ value 
    ),
    mean = case_when(
      n == n2 ~ mean + diff,
      TRUE ~ mean
    ),
    n = factor(n, labels = c("Weak curvature", "Strong curvature"))
  )

ymax <- max(res$value) + 0.25 * abs(max(res$value))
ymin <- min(res$value)

r1 <- 5 * sd(mean_vec1)
r2 <- 8 * sd(mean_vec2)

circle <- tibble(
  n = factor(1:2, labels = c("Weak curvature", "Strong curvature")),  
  xc = c(8, 8),
  a = c(r1, r2),
  b = 500 * c(1, 1 * r2 / r1),
  yc = tmp[1] - b,
  phi = c(0, 0)
) %>% 
  expand_grid(t = seq(0, 2 * pi, length = 100)) %>%
  mutate(x = xc + a * cos(t) * cos(phi) - b * sin(t) * sin(phi),
         y = yc + a * cos(t) * sin(phi) + b * sin(t) * cos(phi))

circle_text <- 
  filter(circle, t > 0.39 & t < 0.45) %>%
  mutate(a2 = xc + a, label = c("r == 1 / kappa", "r"))

my_cols <- RColorBrewer::brewer.pal(3, "Set1")
res |>
  group_by(mu, n) |>
  summarise(mean = mean(mean)) |>
  ggplot(aes(mu, value)) +
  geom_path(data = circle, aes(x, y), col = my_cols[1]) +
  geom_line(aes(y = mean), col = "black", size = 1) +
  geom_vline(xintercept = 8, linetype = "dashed") +
  geom_segment(data = circle_text, aes(xc, yc, xend = x, yend = y), col = my_cols[1]) +
  geom_point(data = circle_text, aes(xc, yc), col = my_cols[1], size = 1.5) +
  geom_text(data = circle_text, aes((xc + x) / 2, (yc + y) / 2 / 0.9, label = label), 
            parse = TRUE, col = my_cols[1]) +
  geom_text(data = circle_text, aes(xc, yc), label = "c", nudge_x = -0.12) +
  scale_x_continuous(breaks = c(6, 7, 8, 9, 10), labels = c("", "", "x", "", "")) +
  labs(y = "f(x)", x = NULL) +
  coord_cartesian(ylim = c(ymin + 800, ymax), xlim = c(6, 10)) +
  facet_grid(. ~ n) +
  theme_bw() + 
  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) 
```

## Curvature and concavity

::: {#def-curvature-alt}
#### Curvature
The (signed) curvature for a graph $y=f(x)$ is
$$
\kappa = \frac{f''(x)}{\big(1 + [f'(x)]^2\big)^{3/2}}.
$$
:::

- The second derivative $f''(x)$ tells us how fast the slope is changing.

- The [**sign**]{.ubdrubydeep} of the curvature is the same as the sign of $f''(x)$. Hence,
  - If $f''(x) > 0$, the graph is **concave up** (convex).
  - If $f''(x) < 0$, the graph is **concave down** (concave).

- The [**magnitude**]{.ubdrubydeep} of the curvature is proportional to $f''(x)$. Hence,
  - If $|f''(x)|$ is large, the graph is *steep* and "curvier".
  - If $|f''(x)|$ is small, the graph is *flat* and "gentle".
  
- For reference, a straight line has zero curvature.

<!-- ```{r} -->
<!-- #| fig-width: 7 -->
<!-- #| fig-height: 3 -->
<!-- #| out-width: 100% -->
<!-- #| label: curvature-concav -->
<!-- ``` -->

## Summary so far

- Derivatives represent rate of change (slope) of a function $f:\mathcal X \to \mathbb R$.

- Interested in optimising an *objective* function $f(x)$ representing some kind of "reward" or "cost".

- Find critical points by solving $f'(x) = 0$.

- Use the second derivative test to classify critical points:
  - If $f''(x) < 0$, then $f$ is concave down at $x$ and $x$ is a local maximum.
  - If $f''(x) > 0$, then $f$ is concave up at $x$ and $x$ is a local minimum.
  - If $f''(x) = 0$, then the test is inconclusive.
  
- Curvature tells us how steep the curve is at its optima. In some sense, it tells us how hard or easy it is to find the optimum.

# A statistical persepctive

## But what *is* statistics?

Statistics is a scientific subject that deals with the collection, analysis, interpretation, and presentation of data.

- **Collection** means designing experiments, questionnaires, sampling schemes, and also administration of data collection.

- **Analysis** means mathematically modelling, estimation, testing, forecasting.

::: {.nudge-up}
![](figures/ppdac.png){width=100% fig-align=center}
:::

::: aside
See also: *The Art of Statistics: Learning from Data* by David Spiegelhalter.
:::

## Motivation

> I toss a coin $n$ times and I wish to find $p$, the probability of heads. Let $X_i=1$ if a heads turns up, and $X_i=0$ if tails.

- I do not know the value of $p$, so I want to estimate it somehow.

- I have a "guess" what it might be e.g. $p=0.5$ or $p=0.7$.

- How do I objectively decide which value is better?

![](figures/bentcoin.png){.absolute right=10 top=200 width="220"}




::: {.callout-note}
#### A more high-stakes example

Think of the binary outcomes as a stock price rising or falling. 
You'll need to decide to invest based on what you believe (or the data suggests) the probability of the stock price rising is.
:::

## A probabilistic model

Each $X_i$ is a *random variable* taking only two possible outcomes, i.e.
$$
X_i = \begin{cases}
1 &\text{w.p. } \ \ p \\
0 &\text{w.p. } \ \ 1-p \\
\end{cases}
$$
This is known as a **Bernoulli** random variable.

. . .

Suppose that $X=X_1 + \dots + X_n$ --- i.e. we are counting the number of heads in $n$ tossess --- then this becomes a **binomial** random variable. We write $X \sim \operatorname{Bin}(n, p)$, and the *probability mass function* is given by
$$
f(x \mid p) = \Pr(X = x) = \binom{n}{x} p^x (1 - p)^{n - x}, \quad x = 0, 1, \ldots, n. 
$$

## Learning from data

::: {.callout-important}
#### $p$ is unknown
If we do not know $p$, then it is not possible to calculate probabilities, expectations, variances... ‚òπÔ∏è
:::

Naturally, you go ahead and collect data by tossing it $n=10$ times. The outcome happens to be
$$
H, H, H, T, T, H, H, T, H, H
$$
There is a total of $X=7$ heads, and from this you surmise that (at least) the coin is *unlikely* to be fair, because:

- If $p=0.5$, then $\Pr(X=7 \mid p = 0.7) = \binom{10}{7} (0.5)^7 (0.5)^3 = 0.117$.
- If $p=0.7$, then $\Pr(X=7 \mid p = 0.5) = \binom{10}{7} (0.7)^7 (0.3)^3 = 0.267$.
- If $p=0.9$, then $\Pr(X=7 \mid p = 0.9) = \binom{10}{7} (0.9)^7 (0.1)^3 = 0.057$.

::: {.nudge-up-large}
How to formalise this idea?
:::

## The likelihood function

::: {#def-likelihood}
Given a probability function $x \mapsto f(x\mid\theta)$ where $x$ is a realisation of a random variable $X$, the *likelihood function* is $\theta \mapsto f(x\mid\theta)$, often written
$\mathcal L(\theta) = f(x \mid \theta)$.
:::

```{r}
#| out-width: 100%
#| fig-height: 3
#| fig-width: 7

x <- seq(0, 1, by = 0.01)
y <- dbinom(7, size = 10, x)

ggplot(data.frame(x = x, y = y), aes(x, y)) +
  geom_line() +
  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +
  labs(x = "Probability of heads, p", y = "L(p|x)",
       subtitle = "Likelihood of Bin(10,p) given fixed data X=7") +
  theme_bw()
```





## The (parametric) statistical model

Assume that each observation $X_i$ is generated from a *probability function* $f$, i.e.
$$
X_i \sim f(x \mid \theta), \quad i = 1, \ldots, n.
$$

Furthermore,

1. Each observation $X_i$ is independent of the others.
2. The functional form of $f$ is known.
3. $\theta$ is the [unknown]{.uline} *parameter* that characterises the distribution $f$.

In other words, the parameter $\theta$ is all that is needed to know about the distribution of $X_i$.

## Examples of parameteric models

::: {.nudge-down-xl}
| Name        | $f(x \mid \theta)$                                                   | $\theta$                                | Remarks                          |
|-------------|----------------------------------------------------------------------|------------------------------------------|----------------------------------|
| Binomial    | $\binom{n}{x} p^x (1 - p)^{n - x}$                                   | $p \in (0, 1)$        | No. successes in $n$ trials        |
| Poisson     | $\frac{\lambda^x e^{-\lambda}}{x!}$                                  | $\lambda > 0$                            | Count data (events per interval) |
| Uniform     | $\frac{1}{b - a}$ for $x \in [a, b]$                                 | $a < b$                                  | Equally likely outcomes     |
| Exponential | $\lambda e^{-\lambda x}$ for $x \geq 0$                              | $\lambda > 0$                            | Waiting time        |
| Normal      | $\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)$ | $\mu \in \mathbb{R},\ \sigma^2 > 0$ | Bell curve       |
|   |   |   |
: {.table-ubd}
:::

## Effect of parameterisation

```{=html}
<div id="desmos-normal" style="position: relative; width: 100%; height: 88%;"></div>

<script>
  var elt = document.getElementById("desmos-normal");
  var calculator = Desmos.GraphingCalculator(elt, {
    expressions: true,
    settingsMenu: false,
    zoomButtons: true
  });

  calculator.setExpressions([
    { id: 'mu', latex: '\\mu = 0', sliderBounds: { min: -5, max: 5 } },
    { id: 'sigma', latex: '\\sigma = 1', sliderBounds: { min: 0.5, max: 3 } },
    { id: 'f', latex: 'f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}' },
    { id: 'graph', latex: 'y = f(x)', color: Desmos.Colors.BLUE }
  ]);

  calculator.setMathBounds({
    left: -8,
    right: 8,
    bottom: -0.05,
    top: 0.5
  });
</script>
```

## When we only have data...

### Probability vs statistics


Given a model, probability allows us to *predict* data.
Statistics on the other hand, allows us to *learn* from data.


::: {.columns}

::: {.column width=48%}

```{tikz}
\usetikzlibrary{fit,positioning,shapes.geometric,decorations.pathreplacing,calc}
\begin{tikzpicture}[scale=0.8, transform shape]
\tikzstyle{obsvar}=[rectangle, thick, minimum size = 10mm,draw =black!80, node distance = 1mm]
\tikzstyle{connect}=[-latex, thick]

\node[obsvar] (fx) [] {$\hspace{1em}f(x|\theta)\hspace{1em}$};
\node (xx) [right=of fx] {\textcolor{blue}{$\{X_1,\dots,X_n\}$}};
\node (theta) [left=of fx] {\textcolor{red}{$\theta$}};
\node (d1) [below=of fx,yshift=9mm] {Model};
\node (d2) [below=of xx,yshift=11mm] {\scriptsize \textcolor{blue}{prob.}};
\node (d3) [below=of theta,yshift=11mm] {\scriptsize \textcolor{red}{param.}};
\node (d1) [above=of fx,yshift=-5mm] {\underline{Probability}};

\path (fx) edge [connect] (xx)
      (theta) edge [connect] (fx);

\end{tikzpicture}
```

- What is $\Pr(X > a)$?
- What is $\operatorname{E}(X)$?


:::

::: {.column width=48%}


```{tikz}
\usetikzlibrary{fit,positioning,shapes.geometric,decorations.pathreplacing,calc}
\begin{tikzpicture}[scale=0.8, transform shape]
\tikzstyle{obsvar}=[rectangle, thick, minimum size = 10mm,draw =black!80, node distance = 1mm]
\tikzstyle{connect}=[-latex, thick]
\node[obsvar] (fx) [] {$\hspace{1em}f(x|\theta)\hspace{1em}$};
\node (xx) [right=of fx] {\textcolor{red}{$\{x_1,\dots,x_n\}$}};
\node (theta) [left=of fx] {\textcolor{blue}{$\hat\theta$}};
\node (d1) [below=of fx,yshift=9mm] {Model};
\node (d2) [below=of xx,yshift=11mm] {\scriptsize \textcolor{red}{data}};
\node (d3) [below=of theta,yshift=11mm] {\scriptsize \textcolor{blue}{est.}};
\node (d1) [above=of fx,yshift=-5mm] {\underline{Statistics}};

\path (fx) edge [connect] (theta)
      (xx) edge [connect] (fx);

\end{tikzpicture}
```

- What is $\theta$?
- Is $\theta$ larger than $\theta_0$?
- How confident am I that $\theta \in (\theta_l,\theta_u)$?

:::

:::


## The likelihood function

::: {#def-likelihood}
The likelihood function is defined as the joint probability of the data $\mathbf X = (X_1,\dots,X_n)^\top$ given the parameter $\theta$, i.e.
$$
L(\theta) = \Pr(\mathbf X \mid \theta) = \prod_{i=1}^n f(X_i \mid \theta).
$$
The log-likelihood is just the logarithm of the likelihood function:
$$
\ell(\theta) = \log L(\theta) = \sum_{i=1}^n \log f(X_i \mid \theta).
$$
:::

The (log-)likelihood function tells us how plausible different values of the parameter $\theta$ are given the observed data $\mathbf X$.


## Example (Normal mean)

Suppose we observe $X_1, \ldots, X_n$ from a normal distribution with unknown mean $\mu$ and known variance $\sigma^2$.
The log-likelihood function is given by
$$
\begin{align*}
\ell(\mu) &= \sum_{i=1}^n \log f(X_i \mid \mu) \\
&= \sum_{i=1}^n \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(X_i - \mu)^2}{2\sigma^2}} \right) \\
&= \sum_{i=1}^n \left\{ -\frac{1}{2} \log(2\pi\sigma^2) - \frac{(X_i - \mu)^2}{2\sigma^2} \right\} \\
&= -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu)^2.
\end{align*}
$$

## Example (Normal mean, cont.)

To find the MLE of $\mu$, we differentiate the log-likelihood function with respect to $\mu$ and set it to zero:
$$
\begin{align*}
\frac{d}{d\mu} \ell(\mu) &= -\frac{1}{2\sigma^2} \cdot 2 \sum_{i=1}^n (X_i - \mu)(-1) \\
&= \frac{1}{\sigma^2} \sum_{i=1}^n (X_i - \mu) = 0 \\
&\Leftrightarrow \sum_{i=1}^n X_i - n\mu = 0 
\Leftrightarrow \mu = \frac{1}{n} \sum_{i=1}^n X_i.
\end{align*}
$$

Thus, the MLE for $\mu$ is $\hat\mu = \bar X = \frac{1}{n} \sum_{i=1}^n X_i$.

## Brunei house price data

```{r}
#| message: false
#| warning: false
#| out-width: 100%
#| fig-height: 4.5
#| fig-width: 8
library(tidyverse)
n <- 100
mu <- 8
x <- rnorm(n, mean = mu, sd = 1)
loglik <- function(mu, X = x) {
  sum(dnorm(X, mean = mu, sd = 1, log = TRUE))
}

xmin <- min(x)
xmax <- max(x)

# Boxplot of sample data
p1 <-
  ggplot(tibble(x, y = 0), aes(x, y)) +
  geom_boxplot() +
  geom_jitter(size = 1.5, alpha = 0.5, height = 0.1) +
  annotate("point", x = mean(x), y = 0, size = 3, colour = "red3") +
  geom_vline(xintercept = mean(x), linetype = "dashed", colour = "red3") +
  theme_minimal() +
  coord_cartesian(ylim = c(-1, 1), xlim = c(xmin, xmax)) +
  theme(axis.text.y= element_blank(), axis.ticks.y = element_blank(),
        axis.text.x = element_blank()) +
  labs(y = NULL, x = NULL, subtitle = "Sampling distribution of data")

# Likelihood function
p2 <- tibble(
    xx = seq(xmin, xmax, length.out = 200),
    # yy = unlist(map(xx, \(z) sum(dnorm(z, mean = mean(x), sd = 1, log = TRUE))))
  ) |>
  rowwise() |>
  mutate(yy = loglik(xx)) |>
  ggplot(aes(xx, yy)) +
  geom_line(size = 1) +
  geom_vline(xintercept = mean(x), linetype = "dashed", colour = "red3") +
  coord_cartesian(xlim = c(xmin, xmax)) +
  labs(x = "x", y = "Log-likelihood") +
  theme_minimal() 

# cowplot::plot_grid(p1, p2, ncol = 1)
library(patchwork)
p1 / p2
```


## But wait, the sample was random...

<Multiple samples, boxplot comparisons>

## Effect of sampling variability on likelihood

## Fisher information 

Second derivative = Fisher Information (intuition)

- Negative expected second derivative of log-likelihood
- Measures how peaked the likelihood is ‚Üí how much information data carries about the parameter


<!-- ## Example (ANOVA) -->

<!-- House price vs district in brunei -->


<!-- ## Example (Wealth data & Pareto distribution) -->

<!-- ## Jacobians and change of variables  -->

<!-- Show why Jacobians matter when transforming distributions. -->
<!-- 	‚Ä¢	Simple change of variable in one dimension -->
<!-- 	‚Ä¢	If $Y = g(X)$, then $f_Y(y) = f_X(x) \left| \frac{dx}{dy} \right|$ -->
<!-- 	‚Ä¢	Multivariate case: use Jacobian determinant -->
<!-- 	‚Ä¢	Example: transforming from Cartesian to polar coordinates -->
<!-- 	‚Ä¢	Example: bivariate normal to standard normal ‚Üí Cholesky or whitening -->

<!-- Application: Why this matters in simulations and Bayesian inference (brief mention of Metropolis-Hastings, normalizing flows, etc.) -->

## Conclusions

- Calculus is not just background math‚Äîit‚Äôs the **engine** of statistical theory.
- Beyond undergraduate stats: everything from MLE, Bayesian posteriors, to machine learning involves gradients and Hessians.
 Suggest: for students who love math but unsure about statistics‚Äîyou are the kind of person statistics needs.

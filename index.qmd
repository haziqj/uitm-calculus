---
title: 'Calculus: Differentiation and Its Application'
subtitle: A statistical perspective
format:
  ubd40-revealjs:
    slide-level: 2
    transition: fade
    auto-stretch: false
    width: 1250  # 1050
    height: 760  # 700
    self-contained: false
    chalkboard: true
    toc: false
    toc-depth: 1
    multiplex: true
    code-block-height: 700px
    # html-table-processing: none
author:
  - name: Dr. Haziq Jamil
    orcid: 0000-0003-3298-1010
    # email: haziq.jamil@ubd.edu.bn | `https://haziqj.ml/aiti-talk`
    affiliations: 
      - 'Assistant Professor in Statistics, Universiti Brunei Darussalam'
      - '<span style="font-style:normal;">[`https://haziqj.ml/uitm-calculus/`](https://haziqj.ml/uitm-calculus/)</span>'
date: 2025-06-14
# bibliography: refs.bib
execute:
  echo: false
  freeze: auto
  cache: false
---

## (Almost) Everything you ought to know...

### ...about calculus in the first year



Let $f:\mathcal X \to \mathbb R$ be a *real-valued* function defined on an input set $\mathcal X$.

::: {.fragment}

::: {#def-derivatives}

#### Differentiability

$f(x)$ is said to be *differentiable* at a point $x \in \mathcal X$ if the limit 

$$
L = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
$${#eq-def-derivative}
exists.
If $L$ exists, we denote it by $f'(x)$ or $\frac{df}{dx}(x)$, and call it the *derivative* of $f$ at $x$.
Further, $f$ is said to be differentiable on $\mathcal X$ if it is differentiable at every point in $\mathcal X$.
:::

:::

::: {.fragment}
For now, we assume $\mathcal X \subseteq \mathbb R$, and will extend to higher dimensions later.
:::

## Some examples


::: {.columns}

::: {.column width="50%"}
<br>

| Function | Derivative |
| --- | --- |
| $f(x) = x^2$ | $f'(x) = 2x$ |
| $f(x) = \sum_{n} a_n x^n$ | $f'(x) = \sum_{n} n a_n x^{n-1}$ | 
| $f(x) = \sin(x)$ | $f'(x) = \cos(x)$ |
| $f(x) = \cos(x)$ | $f'(x) = -\sin(x)$ |
| $f(x) = e^x$ | $f'(x) = e^x$ |
| $f(x) = \ln(x)$ | $f'(x) = \frac{1}{x}$ | 
| | |
: {.table-ubd}
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}

<br>

We can derive it "by hand" using the definition.
Let $f(x) = x^2$.
Then,

<br>
<br>

$$
\begin{align}
\lim_{h \to 0} & \frac{f(x + h) - f(x)}{h} \\
&= \lim_{h \to 0} \frac{(x + h)^2 - x^2}{h} \\
&= \lim_{h \to 0} \frac{x^2 + 2xh + h^2 - x^2}{h} \\[0.5em]
&= \lim_{h \to 0} 2x + h \\[0.5em]
&= 2x.
\end{align}
$$

:::

:::

::: aside
[Taylor series](https://en.wikipedia.org/wiki/Taylor_series) is a powerful tool for approximating functions using derivatives. 
:::


## Graphically...

```{=html}
<script src="https://www.desmos.com/api/v1.11/calculator.js?apiKey=dcb31709b452b1cf9dc26972add0fda6"></script>

<div id="desmos-derivative" style="position: relative; width: 100%; height:88%;"></div>

<script>
  const elt = document.getElementById("desmos-derivative");
  const calculator = Desmos.GraphingCalculator(elt, {
    expressions: true,
    settingsMenu: false,
    zoomButtons: true
  });

  calculator.setExpressions([
    { id: 'f', latex: 'f(x) = x^2' },
    { id: 'a', latex: 'a = 1', sliderBounds: { min: -1, max: 3 } },
    { id: 'h', latex: 'h = 1.5', sliderBounds: { min: 0.001, max: 1.5 } },
    { id: 'pt1', latex: '(a, f(a))', showLabel: true },
    { id: 'pt2', latex: '(a + h, f(a + h))', showLabel: true },
    { id: 's', latex: 'L = \\frac{f(a+h) - f(a)}{h}' },
    { id: 'tangent', latex: 'y = L(x - a) + f(a)', color: Desmos.Colors.RED }
  ]);
  
  calculator.setMathBounds({
    left: -1,
    right: 3,
    bottom: -0.5,
    top: 8
  });

</script>
```

## What is a derivative?

The derivative of a function tells you:    

- üöÄ How fast the function is _changing_ at any point  
- üìê The **slope** of the tangent line at that point

```{r}
#| fig-height: 2.8
#| fig-width: 7
#| out-width: 100%
library(tidyverse)

f <- function(x) -x^2
f_prime <- function(x) -2*x
x_pts <- c(-1, 0, 1)
fty <- factor(c("f increasing", "f stationary", "f decreasing"),
                levels = c("f increasing", "f stationary", "f decreasing"))

curve_df <- tibble(x = seq(-2, 2, length.out = 200), y = f(x))

tangent_df <- lapply(x_pts, function(a) {
  tibble(x = seq(a - 1, a + 1, length.out = 100),
         y = f_prime(a) * x + f(a) - f_prime(a) * a,
         x0 = a,
         type = fty[match(a, x_pts)])
}) |> bind_rows()

point_df <- tibble(x = x_pts, y = f(x_pts), type = fty)

ggplot() +
  geom_line(data = curve_df, aes(x, y), color = "black", linewidth = 0.8) +
  geom_line(data = tangent_df, aes(x, y), color = "red3", linewidth = 0.8) +
  geom_point(data = point_df, aes(x, y), color = "steelblue3", size = 2) +
  facet_wrap(~ type) +
  labs(x = "x", y = "f(x)") +
  theme_bw()
```

## The concept of optimisation

- When $f$ is some kind of a "reward" function, then the value of $x$ that maximises $f$ is highly of interest. Some examples:
  - üí∞ **Profit maximisation**: Find the price that maximises profit.
  - üß¨ **Biological processes**: Find the conditions that maximise growth or reproduction rates.
  - üë∑‚Äç‚ôÇÔ∏è **Engineering**: Find the design parameters that maximise strength or efficiency.


- Derivatives help us find so-called *critical values*: [Solve $f'(x) = 0$]{.ubdrubysoftbg}.

::: {#exm-maxima}

Find the maximum of $f(x) = -3x^4 + 4x^3 + 12x^2$.

:::

. . . 

$$
\begin{align*}
f'(x) = -12x^3 + 12x^2 + 24x &= 0 \\
\Leftrightarrow 12x(2 + x - x^2) &= 0 \\
\Leftrightarrow 12x(x+1)(x-2) &= 0 \\
\Leftrightarrow x &= 0, -1, 2.
\end{align*}
$$

. . . 

::: {.nudge-up-medium}
Are all of these critical values maxima values? ü§î
:::

## Graphically...

```{r}
#| fig-height: 4.1
#| fig-width: 7
#| out-width: 100%
library(ggrepel)

f <- function(x) -3 * x ^ 4 + 4 * x ^ 3 + 12 * x ^ 2#-x^4 + 2*x^3 + 3*x^2 - 2*x
x_crit <- c(-1, 0, 2)
df <- tibble(x = seq(-2, 3, length.out = 500)) %>%
  mutate(y = f(x))

critical_points <- tibble(
  x = x_crit,
  label = c("Local Max", "Local Min", "Global Max"),
  color = RColorBrewer::brewer.pal(3, "Set1")
) %>%
  mutate(y = f(x))

shade_df <- tibble(
  xmin = c(-Inf, x_crit[1], x_crit[2]),
  xmax = c(x_crit[1], x_crit[2], x_crit[3]),
  fill = rep(c("grey75", "white"), length.out = 3)
)

pm_df <- tibble(
  x = ((c(-2.2, x_crit, 3.2) + c(x_crit, 3.2, 0)) / 2)[1:4],
  y = 35
)

ggplot(df, aes(x, y)) +
  geom_rect(data = shade_df, aes(xmin = xmin, xmax = xmax, ymin = -Inf, 
                                 ymax = Inf, fill = fill), 
            inherit.aes = FALSE, alpha = 0.4) +  
  scale_fill_identity() +
  geom_label(data = pm_df, aes(x, y, label = c("+", "‚àí", "+", "‚àí")),
            size = 8, color = "black", vjust = -0.5) +
  geom_line(color = "black", size = 1) +
  geom_point(data = critical_points, aes(x, y, color = label), size = 3,
             show.legend = FALSE) +
  geom_text_repel(
    data = critical_points, 
    aes(x, y, label = label, color = label), 
    size = 4.5, 
    seed = 4, 
    #nudge_y = 0.5,
    box.padding = 1, 
    point.padding = 0.4, 
    show.legend = FALSE
  ) +
  scale_colour_brewer(palette = "Set1") +
  coord_cartesian(ylim = c(-8, 43)) +
  labs(x = "x", y = "f(x)") +
  theme_bw()
```


## How do we know if it's a maxima or minima?

[**Second derivative test:**]{.ubdrubydeep} Measure the **change in slope** around the critical point $\hat x$, i.e. $f''(\hat x) = \frac{d}{dx}\left( \frac{df}{dx}(x) \right) = \frac{d^2f}{dx^2}(x)$.

. . . 

::: {.nudge-down-medium}

| Behaviour of $f$ near $\hat x$               | $f''(\hat x)$       | Shape             | Conclusion     |
|-------------------------------|---------------------|----------------------|-------------------|----------------|
| Increasing ‚Üí Decreasing       | $f''(\hat x) < 0$ | Concave (‚à©)  | Local maximum  |
| Decreasing ‚Üí Increasing       | $f''(\hat x) > 0$ | Convex (‚à™)    | Local minimum  |
| No sign change / flat region  |  $f''(\hat x) = 0$   | Unknown / flat    | Inconclusive   |

:::

## Curvature

::: {#def-curvature}
Let $\mathcal C_x$ denote the *osculating circle* at $x$ with centre $c$ and radius $r$, i.e. the circle that best approximates the graph of $f$ at $x$.
The curvature $\kappa$ for a graph of a function $f$ at a point $x$ is defined as $\kappa = \frac{1}{r}$.
:::

```{r}
#| fig-width: 7
#| fig-height: 3
#| out-width: 100%
#| label: curvature-concav
set.seed(123)
n1 <- 5
n2 <- 500
nratio <- sqrt(n2 / n1)
m <- 100
B <- 200
mu <- c(seq(5.8, 10.2, length = m - 1), 8)
res <- matrix(NA, nrow = m, ncol = B) 
score_vec1 <- score_vec2 <- mean_vec2 <- mean_vec1 <- rep(NA, B)

# Small FI
for (j in 1:B) {
  X <- rnorm(n1, mean = 8, sd = 1)
  ll <- rep(NA, length(mu))
  for (i in seq_along(ll)) 
    ll[i] <- sum(dnorm(X, mean = mu[i], sd = 1, log = TRUE)) 
  res[, j] <- ll
  mean_vec1[j] <- mean(X)
  score_vec1[j] <- sum(X - 8)
}
colnames(res) <- 1:B
res1 <-
  as_tibble(nratio * res) %>%
  mutate(mu = mu,
         mean = apply(nratio * res, 1, mean),
         n = n1) 

# Big FI
for (j in 1:B) {
  X <- rnorm(n2, mean = 8, sd = 1)
  ll <- rep(NA, length(mu))
  for (i in seq_along(ll)) 
    ll[i] <- sum(dnorm(X, mean = mu[i], sd = 1, log = TRUE)) 
  res[, j] <- ll
  mean_vec2[j] <- mean(X)
  score_vec2[j] <- sum(X - 8)
}
colnames(res) <- 1:B
res2 <-
  as_tibble(res) %>%
  mutate(mu = mu,
         mean = apply(res, 1, mean),
         n = n2) 

res <- bind_rows(res1, res2) %>%
  pivot_longer(`1`:`B`)

tmp <- res %>% filter(mu == 8) %>% summarise(mean = unique(mean)) %>% unlist()
diff <- max(tmp) - min(tmp)

res <-
  res |>
  mutate(
    value = case_when(
      n == n2 ~ value + diff,
      TRUE ~ value 
    ),
    mean = case_when(
      n == n2 ~ mean + diff,
      TRUE ~ mean
    ),
    n = factor(n, labels = c("Weak curvature", "Strong curvature"))
  )

ymax <- max(res$value) + 0.25 * abs(max(res$value))
ymin <- min(res$value)

r1 <- 5 * sd(mean_vec1)
r2 <- 8 * sd(mean_vec2)

circle <- tibble(
  n = factor(1:2, labels = c("Weak curvature", "Strong curvature")),  
  xc = c(8, 8),
  a = c(r1, r2),
  b = 500 * c(1, 1 * r2 / r1),
  yc = tmp[1] - b,
  phi = c(0, 0)
) %>% 
  expand_grid(t = seq(0, 2 * pi, length = 100)) %>%
  mutate(x = xc + a * cos(t) * cos(phi) - b * sin(t) * sin(phi),
         y = yc + a * cos(t) * sin(phi) + b * sin(t) * cos(phi))

circle_text <- 
  filter(circle, t > 0.39 & t < 0.45) %>%
  mutate(a2 = xc + a, label = c("r == 1 / kappa", "r"))

my_cols <- RColorBrewer::brewer.pal(3, "Set1")
res |>
  group_by(mu, n) |>
  summarise(mean = mean(mean)) |>
  ggplot(aes(mu, value)) +
  geom_path(data = circle, aes(x, y), col = my_cols[1]) +
  geom_line(aes(y = mean), col = "black", size = 1) +
  geom_vline(xintercept = 8, linetype = "dashed") +
  geom_segment(data = circle_text, aes(xc, yc, xend = x, yend = y), col = my_cols[1]) +
  geom_point(data = circle_text, aes(xc, yc), col = my_cols[1], size = 1.5) +
  geom_text(data = circle_text, aes((xc + x) / 2, (yc + y) / 2 / 0.9, label = label), 
            parse = TRUE, col = my_cols[1]) +
  geom_text(data = circle_text, aes(xc, yc), label = "c", nudge_x = -0.12) +
  scale_x_continuous(breaks = c(6, 7, 8, 9, 10), labels = c("", "", "x", "", "")) +
  labs(y = "f(x)", x = NULL) +
  coord_cartesian(ylim = c(ymin + 800, ymax), xlim = c(6, 10)) +
  facet_grid(. ~ n) +
  theme_bw() + 
  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) 
```

## Curvature and concavity

::: {#def-curvature-alt}
#### Curvature
The (signed) curvature for a graph $y=f(x)$ is
$$
\kappa = \frac{f''(x)}{\big(1 + [f'(x)]^2\big)^{3/2}}.
$$
:::

- The second derivative $f''(x)$ tells us how fast the slope is changing.

- The [**sign**]{.ubdrubydeep} of the curvature is the same as the sign of $f''(x)$. Hence,
  - If $f''(x) > 0$, the graph is **concave up** (convex).
  - If $f''(x) < 0$, the graph is **concave down** (concave).

- The [**magnitude**]{.ubdrubydeep} of the curvature is proportional to $f''(x)$. Hence,
  - If $|f''(x)|$ is large, the graph is *steep* and "curvier".
  - If $|f''(x)|$ is small, the graph is *flat* and "gentle".
  
- For reference, a straight line has zero curvature.

<!-- ```{r} -->
<!-- #| fig-width: 7 -->
<!-- #| fig-height: 3 -->
<!-- #| out-width: 100% -->
<!-- #| label: curvature-concav -->
<!-- ``` -->

## Summary so far

- Derivatives represent rate of change (slope) of a function $f:\mathcal X \to \mathbb R$.

- Interested in optimising an *objective* function $f(x)$ representing some kind of "reward" or "cost".

- Find critical points by solving $f'(x) = 0$.

- Use the second derivative test to classify critical points:
  - If $f''(x) < 0$, then $f$ is concave down at $x$ and $x$ is a local maximum.
  - If $f''(x) > 0$, then $f$ is concave up at $x$ and $x$ is a local minimum.
  - If $f''(x) = 0$, then the test is inconclusive.
  
- Curvature tells us how steep the curve is at its optima. In some sense, it tells us how hard or easy it is to find the optimum.

# A statistical persepctive

## Motivation

Show students how theoretical tools they‚Äôre learning now are used to derive estimators;


Plant the idea that Statistics isn‚Äôt just ‚Äúdata‚Äù or ‚ÄúExcel‚Äù, but has real mathematical depth;
Give a clear reason to care about derivatives, gradients, Hessians, and Jacobians in practice.

## Functions



## Derivatives



## Derivatives

Goal: Shift mindset from high-school calculus to tools for optimization and multidimensional problems.
	‚Ä¢	Quick recap: derivative as slope, second derivative as curvature
	‚Ä¢	Extension to multivariable functions:
	‚Ä¢	Partial derivatives
	‚Ä¢	Gradient as direction of steepest ascent
	‚Ä¢	Hessian as local curvature matrix
	‚Ä¢	Applications in optimization: what it means to maximize or minimize a function with multiple variables

Example: A simple multivariate function $f(x, y) = -x^2 - y^2 + 4x + 6y$.
Find its critical point using partial derivatives, use the Hessian to classify.


## Maximum likelihood estimation

- Define likelihood: $L(\theta) = \prod_{i=1}^n f(x_i \mid \theta)$
  - Log-likelihood: easier to differentiate
  - MLE found by solving $\frac{d}{d\theta} \ell(\theta) = 0$

## Examples

Emphasize: the maximum is found because second derivative is negative (concavity)

## Fisher information 

Second derivative = Fisher Information (intuition)

- Negative expected second derivative of log-likelihood
- Measures how peaked the likelihood is ‚Üí how much information data carries about the parameter

## Jacobians and change of variables 

Show why Jacobians matter when transforming distributions.
	‚Ä¢	Simple change of variable in one dimension
	‚Ä¢	If $Y = g(X)$, then $f_Y(y) = f_X(x) \left| \frac{dx}{dy} \right|$
	‚Ä¢	Multivariate case: use Jacobian determinant
	‚Ä¢	Example: transforming from Cartesian to polar coordinates
	‚Ä¢	Example: bivariate normal to standard normal ‚Üí Cholesky or whitening

Application: Why this matters in simulations and Bayesian inference (brief mention of Metropolis-Hastings, normalizing flows, etc.)

## Conclusions

- Calculus is not just background math‚Äîit‚Äôs the **engine** of statistical theory.
- Beyond undergraduate stats: everything from MLE, Bayesian posteriors, to machine learning involves gradients and Hessians.
 Suggest: for students who love math but unsure about statistics‚Äîyou are the kind of person statistics needs.
